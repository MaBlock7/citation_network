literature_titles = {
    "Graph of Thoughts: Solving Elaborate Problems with Large Language Models": [
        "Practice of Streaming Processing of Dynamic Graphs: Concepts, Models, and Systems",
        "GDI: A Graph Database Interface Standard",
        "The Graph Database Interface: Scaling Online Transactional and Analytical Graph Workloads to Hundreds of Thousands of Cores",
        "Demystifying Graph Databases: Analysis and Taxonomy of Data Organization, System Designs, and Graph Queries",
        "Motif Prediction with Graph Neural Networks",
        "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis",
        "Neural Graph Databases",
        "SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems",
        "Communication-Efficient Jaccard Similarity for HighPerformance Distributed Genome Comparisons",
        "ProbGraph: High-Performance and High-Accuracy Graph Mining with Probabilistic Set Representations",
        "GraphMineSuite: Enabling HighPerformance and Programmable Graph Mining Algorithms with Set Algebra",
        "Geometric Deep Learning: Going beyond Euclidean data",
        "Language Models are Few-Shot Learners",
        "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
        "Graph Mining: Laws, Generators, and Algorithms",
        "Machine Learning on Graphs: A Model and Comprehensive Taxonomy",
        "Teaching Large Language Models to Self-Debug",
        "Fast Graph Pattern Matching",
        "PaLM: Scaling Language Modeling with Pathways",
        "Mining Graph Data",
        "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
        "LowLatency Graph Streaming Using Compressed PurelyFunctional Trees",
        "Language Model Cascades",
        "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
        "Graph Pattern Matching: From Intractable to Polynomial Time",
        "DISTINGER: A distributed graph data structure for massive dynamic graph processing",
        "Triangles to Capture Social Cohesion",
        "Hierarchical Models in the Brain",
        "Complexity-Based Prompting for Multi-Step Reasoning",
        "Learning Combinatorial Node Labeling Algorithms",
        "Lifting Sequential Graph Algorithms for Distributed-Memory Parallel Computation",
        "The Parallel BGL: A generic library for distributed graph computations",
        "Representation Learning on Graphs: Methods and Applications",
        "A survey on improving NLP models with human explanations",
        "Cyclic Pattern Kernels for Predictive Graph Mining",
        "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
        "Inner Monologue: Embodied Reasoning through Planning with Language Models",
        "A survey of frequent subgraph mining algorithms",
        "Language Models can Solve Computer Tasks",
        "Explanation-Based Human Debugging of NLP Models: A Survey",
        "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "Large Language Model Guided Treeof-Thought",
        "Challenges in Parallel Graph Processing",
        "Self-Refine: Iterative Refinement with Self-Feedback",
        "Pregel: A System for Large-Scale Graph Processing",
        "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding",
        "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
        "REFINER: Reasoning Feedback on Intermediate Representations",
        "Shaping Communities out of Triangles",
        "Reasoning with Language Model Prompting: A Survey",
        "graph-of-thoughts Repository",
        "Improving Language Understanding by Generative Pre-Training",
        "Language Models are Unsupervised Multitask Learners",
        "Graph Databases: New Opportunities for Connected Data",
        "The Future is Big Graphs: A Community View on Graph Processing Systems",
        "The Graph Neural Network Model",
        "Graph clustering",
        "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
        "Reflexion: Language Agents with Verbal Reinforcement Learning",
        "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
        "Arabesque: A System for Distributed Graph Mining",
        "LLaMA: Open and Efficient Foundation Language Models",
        "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "Attention is All you Need",
        "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
        "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
        "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
        "Interactive Natural Language Processing",
        "Putting Humans in the Natural Language Processing Loop: A Survey",
        "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "PromptChainer: Chaining Large Language Model Prompts through Visual Programming",
        "AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts",
        "A Comprehensive Survey on Graph Neural Networks",
        "Self-Evaluation Guided Beam Search for Reasoning",
        "Foundation Models for Decision Making: Problems, Methods, and Opportunities",
        "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "ReAct: Synergizing Reasoning and Acting in Language Models",
        "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models",
        "STaR: Bootstrapping Reasoning With Reasoning",
        "Planning with Large Language Models for Code Generation",
        "Deep Learning on Graphs: A Survey",
        "Graph neural networks: A review of methods and applications",
        "Large Language Models Are Human-Level Prompt Engineers",
        "Solving Math Word Problems via Cooperative Reasoning induced Language Models"
    ]
}