literature_titles = {
    "TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS": [
        "Palm 2 technical report",
        "Language models are few-shot learners",
        "A dataset for answering time-sensitive questions",
        "Palm: Scaling language modeling with pathways",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies",
        "Measuring massive multitask language understanding",
        "Training compute-optimal large language models",
        "Scaling laws for neural language models",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "Large language models are zero-shot reasoners",
        "Draw me a flower: Processing and grounding abstraction in natural language",
        "Let's verify step by step",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Reframing instructional prompts to gptk's language",
        "Cross-task generalization via natural language crowdsourcing instructions",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Gpt-4 technical report",
        "Don't blame the annotator: Bias already starts in the annotation instructions",
        "Is a question decomposition unit all we need?",
        "Measuring and narrowing the compositionality gap in language models",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Kepler's laws of planetary motion: 1609â€“1666",
        "Recitation-augmented language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Musique: Multihop questions via single-hop question composition",
        "Attention is all you need",
        "Finetuned language models are zero-shot learners",
        "Emergent abilities of large language models",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Large language models as optimizers",
        "Situatedqa: Incorporating extra-linguistic contexts into qa",
        "Calibrate before use: Improving few-shot performance of language models",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}