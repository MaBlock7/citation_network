literature_titles = {
    "Chain of Hindsight aligns Language Models with Feedback": [
        "Hindsight experience replay",
        "A general language assistant as a laboratory for alignment",
        "An actor-critic algorithm for sequence prediction",
        "Training a helpful and harmless assistant with reinforcement learning from human feedback",
        "Constitutional ai: Harmlessness from ai feedback",
        "Better rewards yield better summaries: Learning to summarise without references",
        "Language models are few-shot learners",
        "Decision transformer: Reinforcement learning via sequence modeling",
        "Towards coherent and cohesive long-form text generation",
        "Deep reinforcement learning from human preferences",
        "Scaling instruction-finetuned language models",
        "Hierarchical neural story generation",
        "Controlling linguistic style aspects in neural language generation",
        "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
        "The Pile: An 800GB dataset of diverse text for language modeling",
        "Scaling laws for reward model overoptimization",
        "Koala: A dialogue model for academic research",
        "Learning from dialogue after deployment: Feed yourself, chatbot!",
        "Large language models can self-improve",
        "Learning to achieve goals",
        "Ctrl: A conditional transformer language model for controllable generation",
        "Adam: A method for stochastic optimization",
        "Pretraining language models with human preferences",
        "Can neural machine translation be improved with user feedback?",
        "In-context reinforcement learning with algorithm distillation",
        "Improving a neural semantic parser by counterfactual learning from human bandit feedback",
        "Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training",
        "Evaluating human-language model interaction",
        "Don't say that! making inconsistent dialogue unlikely with unlikelihood training",
        "Fcm: Forgetful causal masking makes causal language models better zero-shot learners",
        "QUARK: Controllable text generation with reinforced unlearning",
        "Interactive learning from policy-dependent human feedback",
        "Cross-task generalization via natural language crowdsourcing instructions",
        "Webgpt: Browser-assisted question-answering with human feedback",
        "Training language models to follow instructions with human feedback",
        "Finding generalizable evidence by learning to convince q&a models",
        "Improving language understanding by generative pre-training",
        "Language models are unsupervised multitask learners",
        "Multitask prompted training enables zero-shot task generalization",
        "Universal value function approximators",
        "Training language models with language feedback",
        "Chatgpt: Optimizing language models for dialogue",
        "Proximal policy optimization algorithms",
        "Dropout: a simple way to prevent neural networks from overfitting",
        "Learning to summarize with human feedback",
        "Llama: Open and efficient foundation language models",
        "Attention is all you need",
        "Tl; dr: Mining reddit to learn automatic summarization",
        "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
        "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
        "Deep tamer: Interactive agent shaping in high-dimensional state spaces",
        "Finetuned language models are zero-shot learners",
        "Chain of thought prompting elicits reasoning in large language models",
        "Neural text generation with unlikelihood training",
        "Crossfit: A few-shot learning challenge for cross-task generalization in nlp",
        "Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators",
        "Star: Self-taught reasoner bootstrapping reasoning with reasoning",
        "Opt: Open pre-trained transformer language models",
        "The wisdom of hindsight makes language models better instruction followers",
        "Learning to compare for better training and evaluation of open domain natural language generation models",
        "Fine-tuning language models from human preferences"
    ]
}