literature_titles = {
    "Metacognitive Prompting Improves Understanding in Large Language Models": [
        "KBot: a Knowledge graph based chatBot for natural language understanding over linked data",
        "Natural language understanding",
        "Palm 2 technical report",
        "Spoken language understanding for natural interaction: The siri experience",
        "Language models are few-shot learners",
        "Jumping NLP curves: A review of natural language processing research",
        "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "The pascal recognising textual entailment challenge",
        "The commitmentbank: Investigating projection in naturally occurring discourse",
        "Foundations of computational linguistics",
        "Towards reasoning in large language models: A survey",
        "Large language models are zero-shot reasoners",
        "The winograd schema challenge",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Recent advances in natural language processing via large pre-trained language models: A survey",
        "Language model is all you need: Natural language understanding as question answering",
        "What can we learn from collective human opinions on natural language inference data?",
        "GPT-4 Technical Report",
        "Cognitive modules of an NLP knowledge base for language understanding",
        "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "Squad: 100,000+ questions for machine comprehension of text",
        "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
        "Metacognition",
        "First quora dataset release: question pairs (2017)",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Use chat gpt to solve programming bugs",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Superglue: A stickier benchmark for general-purpose language understanding systems",
        "Glue: A multi-task benchmark and analysis platform for natural language understanding",
        "SelfConsistency Improves Chain of Thought Reasoning in Language Models",
        "Integrating Physiological Time Series and Clinical Notes with Transformer for Early Prediction of Sepsis",
        "Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding",
        "An empirical study on the robustness of the segment anything model (sam)",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Ethical and social risks of harm from language models",
        "Automatic chain of thought prompting in large language models",
        "A survey of large language models",
        "Empirical quantitative analysis of covid-19 forecasting models",
        "Progressive-hint prompting improves reasoning in large language models",
        "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
        "Exploring ai ethics of chatgpt: A diagnostic analysis"
    ]
}