literature_titles = {
    "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models": [
        "Language models are few-shot learners",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "PaLM: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "Successive prompting for decomposing complex questions",
        "Complexity-based prompting for multi-step reasoning",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Towards a unified view of parameter-efficient transfer learning",
        "Measuring mathematical problem solving with the math dataset",
        "Learning to solve arithmetic word problems with verb categorization",
        "Parameter-efficient transfer learning for nlp",
        "Towards reasoning in large language models: A survey",
        "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "Large language models are zero-shot reasoners",
        "Parsing algebraic word problems into equations",
        "MAWPS: A math word problem repository",
        "On the advance of making language models better reasoners",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "Llm+ p: Empowering large language models with optimal planning proficiency",
        "Roberta: A robustly optimized bert pretraining approach",
        "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
        "Are NLP models really able to solve simple math word problems?",
        "Measuring and narrowing the compositionality gap in language models",
        "Solving general arithmetic word problems",
        "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
        "On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning",
        "Pearl: Prompting large language models to plan and execute actions over long documents",
        "Black-box tuning for language-model-as-a-service",
        "Large language models can self-improve",
        "Challenging big-bench tasks and whether chain-of-thought can solve them",
        "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
        "Lamda: Language models for dialog applications",
        "Towards understanding chain-of-thought prompting: An empirical study of what matters",
        "Self-consistency improves chain of thought reasoning in language models",
        "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Large language models are reasoners with self-verification",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "React: Synergizing reasoning and acting in language models",
        "Automatic chain of thought prompting in large language models",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}