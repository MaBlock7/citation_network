literature_titles = {
    "Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting": [
        "Explanations for CommonsenseQA: New Dataset and Models",
        "Language models are few-shot learners",
        "e-snli: Natural language inference with natural language explanations",
        "Evaluating large language models trained on code",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
        "Black-box prompt learning for pre-trained language models",
        "Complexity-based prompting for multi-step reasoning",
        "Pal: Program-aided language models",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Demystifying prompts in language models via perplexity estimation",
        "Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification",
        "Large language models can self-improve",
        "Maieutic prompting: Logically consistent reasoning with recursive explanations",
        "Large language models are zero-shot reasoners",
        "MAWPS: A math word problem repository",
        "Can language models learn from explanations in context?",
        "Explanations from large language models make small reasoners better",
        "On the advance of making language models better reasoners",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Text and patterns: For effective chain of thought, it takes two to tango",
        "Few-shot self-rationalization with natural language prompts",
        "Reframing instructional prompts to GPTk's language",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Training language models to follow instructions with human feedback",
        "Are NLP models really able to solve simple math word problems?",
        "Grips: Gradient-free, edit-based instruction search for prompting large language models",
        "Prompt programming for large language models: Beyond the few-shot paradigm",
        "Learning to retrieve prompts for in-context learning",
        "Language models are multilingual chain-of-thought reasoners",
        "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
        "Make prompt-based black-box tuning colorful: Boosting model generalization from three orthogonal perspectives",
        "BBTv2: Towards a gradient-free future with large language models",
        "Black-box tuning for language-model-as-a-service",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Towards understanding chain-of-thought prompting: An empirical study of what matters",
        "Pinto: Faithful language reasoning using prompt-generated rationales",
        "Rationale-augmented ensembles in language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "The unreliability of explanations in few-shot prompting for textual reasoning",
        "Complementary explanations for effective in-context learning",
        "Star: Bootstrapping reasoning with reasoning",
        "Tempera: Test-time prompting via reinforcement learning",
        "Automatic chain of thought prompting in large language models",
        "Least-to-most prompting enables complex reasoning in large language models",
        "Large language models are human-level prompt engineers"
    ]
}