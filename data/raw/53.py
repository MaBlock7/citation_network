literature_titles = {
    "Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations": [
        "Towards a human-like open-domain chatbot",
        "Maximum satisfiability problem",
        "Abductive commonsense reasoning",
        "Flexible generation of natural language deductions",
        "Learning to rationalize for nonmonotonic reasoning with distant supervision",
        "Language models are few-shot learners",
        "e-snli: Natural language inference with natural language explanations",
        "Can rationalization improve robustness?",
        "Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension",
        "Training verifiers to solve math word problems",
        "Towards teachable reasoning systems",
        "Techniques for interpretable machine learning",
        "Measuring and improving consistency in pretrained language models",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Explaining self-explaining: A contrast between content and generation",
        "The curious case of neural text degeneration",
        "Surface form competition: Why the highest probability answer isn't always right",
        "Contrastive explanations for model interpretability",
        "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
        "Beliefbank: Adding memory to a pre-trained language model for a systematic notion of belief",
        "Unifiedqa: Crossing format boundaries with a single qa system",
        "Self-attention between datapoints: Going beyond individual input-output pairs in deep learning",
        "Computing krippendorff's alpha-reliability",
        "Can language models learn from explanations in context?",
        "Generated knowledge prompting for commonsense reasoning",
        "Roberta: A robustly optimized bert pretraining approach",
        "Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark",
        "Neurologic decoding: (un)supervised neural text generation with predicate logic constraints",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Adversarially regularising neural nli models to integrate logical background knowledge",
        "Core-guided maxsat with soft cardinality constraints",
        "Wt5?! training text-to-text models to explain their predictions",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning",
        "Creak: A dataset for commonsense reasoning over entity knowledge",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Explain yourself! leveraging language models for commonsense reasoning",
        "Unsupervised commonsense question answering with self-talk",
        "COM2SENSE: A commonsense reasoning benchmark with complementary sentences",
        "CommonsenseQA 2.0: Exposing the limits of AI through gamification",
        "Socrates, ironist and moral philosopher",
        "What if we simply swap the two text fragments? a straightforward yet effective way to test the robustness of methods to confounding signals in nature language inference tasks",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Consistency of a recurrent language model with respect to incomplete decoding",
        "Teach me to explain: A review of datasets for explainable nlp",
        "A broad-coverage challenge corpus for sentence understanding through inference",
        "The unreliability of explanations in few-shot in-context learning",
        "Calibrate before use: Improving few-shot performance of language models"
    ]
}