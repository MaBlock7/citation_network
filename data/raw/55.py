literature_titles = {
    "Measuring and Narrowing the Compositionality Gap in Language Models": [
        "Thinking aloud: Dynamic context generation improves zero-shot reasoning performance of gpt-2",
        "Adaptive neural networks for efficient inference",
        "Improving language models by retrieving from trillions of tokens",
        "Language models are few-shot learners",
        "Ask the right questions: Active question reformulation with reinforcement learning",
        "The second conversational intelligence challenge (convai2)",
        "Neural logic machines",
        "Towards a human-like open-domain chatbot",
        "Adaptive computation time for recurrent neural networks",
        "Realm: Retrievalaugmented language model pre-training",
        "Constructing a multihop QA dataset for comprehensive evaluation of reasoning steps",
        "Multi-scale dense networks for resource efficient image classification",
        "Compositionality decomposed: How do neural networks generalise?",
        "Search-based neural structured learning for sequential question answering",
        "Leveraging passage retrieval with generative models for open domain question answering",
        "Realtime qa: What's the answer right now?",
        "Measuring compositional generalization: A comprehensive method on realistic data",
        "Generalization through memorization: Nearest neighbor language models",
        "Text modular networks: Learning to decompose tasks in the language of existing models",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "Large language models are zero-shot reasoners",
        "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
        "Retrieval-augmented generation for knowledgeintensive nlp tasks",
        "A diversity-promoting objective function for neural conversation models",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "Generated knowledge prompting for commonsense reasoning",
        "Teaching language models to support answers with verified quotes",
        "Multi-hop reading comprehension through question decomposition and rescoring",
        "Reframing instructional prompts to GPTk's language",
        "Webgpt: Browserassisted question-answering with human feedback",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Is a question decomposition unit all we need?",
        "Unsupervised question decomposition for question answering",
        "Answering complex open-domain questions through iterative query generation",
        "Answer-based Adversarial Training for Generating Clarification Questions",
        "How much knowledge can you pack into the parameters of a language model?",
        "Closed ai models make bad baselines",
        "Recipes for building an open-domain chatbot",
        "Knowledge-aware language model pretraining",
        "The right tool for the job: Matching model and instance complexities",
        "Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks",
        "Neural speed reading via skim-rnn",
        "Generative deep neural networks for dialogue: A short review",
        "Neural responding machine for short-text conversation",
        "Unsupervised commonsense question answering with selftalk",
        "A neural network approach to context-sensitive generation of conversational responses",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "The web as a knowledge-base for answering complex questions",
        "olmpics-on what language model pre-training captures",
        "Lamda: Language models for dialog applications",
        "Musique: Multihop questions via single-hop question composition",
        "A neural conversational model",
        "Shepherd pre-trained language models to develop a train of thought: An iterative prompting approach",
        "Skipnet: Learning dynamic routing in convolutional networks",
        "Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Break it down: A question understanding benchmark",
        "React: Synergizing reasoning and acting in language models",
        "The unreliability of explanations in few-shot in-context learning",
        "Personalizing dialogue agents: I have a dog, do you have pets too?",
        "Leastto-most prompting enables complex reasoning in large language models"
    ]
}