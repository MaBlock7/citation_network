literature_titles = {
    "STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning": [
        "The principles of psychology, volume 1",
        "Protocol analysis: Verbal reports as data",
        "Explain yourself! leveraging language models for commonsense reasoning",
        "Unsupervised commonsense question answering with self-talk",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Few-shot self-rationalization with natural language prompts",
        "Can language models learn from explanations in context?",
        "Training verifiers to solve math word problems",
        "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
        "Language models are few-shot learners",
        "Finetuned language models are zero-shot learners",
        "An explanation of in-context learning as implicit bayesian inference",
        "In-context learning and induction heads",
        "The power of scale for parameter-efficient prompt tuning",
        "Formal mathematics statement curriculum learning",
        "The lean theorem prover (system description)",
        "Generative language modeling for automated theorem proving",
        "Towards interpretable natural language understanding with explanations as latent variables",
        "Sub-task decomposition enables learning in sequence to sequence tasks",
        "Thinking fast and slow with deep learning and tree search",
        "Iterated learning for emergent systematicity in vqa",
        "Learning context-dependent mappings from sentences to logical form",
        "Weakly supervised semantic parsing with abstract examples",
        "From language to programs: Bridging reinforcement learning and maximum marginal likelihood",
        "e-snli: Natural language inference with natural language explanations",
        "Generate natural language explanations for recommendation",
        "Proximal policy optimization algorithms",
        "Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Conceptnet 5.5: An open multilingual graph of general knowledge",
        "Human parity on commonsenseqa: Augmenting self-attention with external attention",
        "Lamda: Language models for dialog applications",
        "Prolific.acâ€”a subject pool for online experiments",
        "Selfconsistency improves chain of thought reasoning in language models",
        "Explanation in artificial intelligence: Insights from the social sciences",
        "The promise and peril of human evaluation for model interpretability",
        "Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness?",
        "Palm: Scaling language modeling with pathways",
        "Solving quantitative reasoning problems with language models",
        "The pile: An 800gb dataset of diverse text for language modeling",
        "Adam: A method for stochastic optimization"
    ]
}