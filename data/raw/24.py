literature_titles = {
    "CONNECTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS": [
        "Asset: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations",
        "Promptsource: An integrated development environment and repository for natural language prompts",
        "Self-adapting control parameters in differential evolution: A comparative study on numerical benchmark problems",
        "Language models are few-shot learners",
        "Evoprompting: Language models for code-level neural architecture search",
        "Introduction to derivative-free optimization",
        "Differential evolution: A survey of the state-of-the-art",
        "Recent advances in differential evolution-an updated survey",
        "RLPrompt: Optimizing discrete text prompts with reinforcement learning",
        "Ant colony system: a cooperative learning approach to the traveling salesman problem",
        "Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance",
        "Samsum corpus: A human-annotated dialogue dataset for abstractive summarization",
        "Learning to program with natural language",
        "Adaptation in Natural and Artificial Systems",
        "Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence",
        "Mining and summarizing customer reviews",
        "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
        "How can we know what language models know?",
        "Particle swarm optimization",
        "Large language models are zero-shot reasoners",
        "Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design",
        "Evolution through large models",
        "The power of scale for parameter-efficient prompt tuning",
        "Deliberate then generate: Enhanced prompting framework for text generation",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "Roulette-wheel selection via stochastic acceptance",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
        "Gpt understands, too",
        "Language model crossover: Variation through few-shot prompting",
        "Genetic algorithm: Theory, literature review, and application in image reconstruction",
        "Reframing instructional prompts to gptk's language",
        "Cross-task generalization via natural language crowdsourcing instructions",
        "An introduction to genetic algorithms",
        "Illuminating search spaces by mapping elites",
        "Gpt-4 technical report",
        "Training language models to follow instructions with human feedback",
        "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
        "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
        "Differential evolution: A review of more than two decades of research",
        "Grips: Gradient-free, edit-based instruction search for prompting large language models",
        "Differential evolution",
        "Automatic prompt optimization with 'gradient descent' and beam search",
        "Is chatgpt a general-purpose natural language processing task solver?",
        "Derivative-free optimization: a review of algorithms and comparison of software implementations",
        "Multitask prompted training enables zero-shot task generalization",
        "Exploiting cloze-questions for few-shot text classification and natural language inference",
        "A thorough examination of decoding methods in the era of llms",
        "Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?",
        "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces",
        "Challenging big-bench tasks and whether chain-of-thought can solve them",
        "Stanford alpaca: An instruction-following llama model",
        "Llama: Open and efficient foundation language models",
        "A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems",
        "Building a question answering test collection",
        "Universal adversarial triggers for attacking and analyzing nlp",
        "Hint-enhanced in-context learning wakes large language models up for knowledge-intensive tasks",
        "Tournament selection â€” Wikipedia, the free encyclopedia",
        "Optimizing statistical machine translation for text simplification",
        "Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts",
        "Jade: Adaptive differential evolution with optional external archive",
        "Differentiable prompt makes pre-trained language models better few-shot learners",
        "Opt: Open pre-trained transformer language models",
        "Tempera: Test-time prompt editing via reinforcement learning",
        "Sentiment analysis in the era of large language models: A reality check",
        "Character-level convolutional networks for text classification",
        "Multi-task instruction tuning of llama for specific scenarios: A preliminary study on writing assistance",
        "Can gpt-4 perform neural architecture search?",
        "Large language models are human-level prompt engineers",
        "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts"
    ]
}
