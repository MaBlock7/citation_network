literature_titles = {
    "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models": [
        "Types of out-of-distribution texts and how to detect them",
        "Token merging: Your vit but faster",
        "Adapting language models to compress contexts",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "Training verifiers to solve math word problems",
        "Language modeling is compression",
        "GPT3.int8(): 8-bit matrix multiplication for transformers at scale",
        "SparseGPT: Massive language models can be accurately pruned in one-shot",
        "OPTQ: Accurate quantization for generative pre-trained transformers",
        "Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance",
        "Complexity-based prompting for multi-step reasoning",
        "Extensible prompts for language models",
        "In-context autoencoder for context compression in a large language model",
        "Semantic compression with large language models",
        "Power-bert: Accelerating BERT inference via progressive word-vector elimination",
        "LoRA: Low-rank adaptation of large language models",
        "Lengthadaptive transformer: Train once with length drop, use anytime with search",
        "Learned token pruning for transformers",
        "Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering",
        "ROUGE: A package for automatic evaluation of summaries",
        "Decoupled weight decay regularization",
        "Self-supervised losses for one-class textual anomaly detection",
        "AdapLeR: Speeding up inference by adaptive length reduction",
        "Learning to compress prompts with gist tokens",
        "Bleu: a method for automatic evaluation of machine translation",
        "Source coding algorithms for fast data compression",
        "Dynamicvit: Efficient vision transformers with dynamic token sparsification",
        "Generalized kraft inequality and arithmetic coding",
        "Prediction and entropy of printed english",
        "A theory of unsupervised learning",
        "Challenging big-bench tasks and whether chain-of-thought can solve them",
        "Stanford alpaca: An instruction-following llama model",
        "Chain of thought prompting elicits reasoning in large language models",
        "Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models",
        "Multi-level knowledge distillation for out-of-distribution detection in text",
        "Smoothquant: Accurate and efficient post-training quantization for large language models",
        "Wizardlm: Empowering large language models to follow complex instructions",
        "Inference with reference: Lossless acceleration of large language models",
        "Xlnet: Generalized autoregressive pretraining for language understanding",
        "Mlcopilot: Unleashing the power of large language models in solving machine learning tasks",
        "Bertscore: Evaluating text generation with BERT",
        "Efficient prompting via dynamic in-context learning"
    ]
}