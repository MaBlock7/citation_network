literature_titles = {
    "Compositional Exemplars for In-context Learning":

[
    "In-context examples selection for machine translation",
    "Cont: Contrastive neural text generation",
    "Task-oriented dialogue as dataflow synthesis",
    "Learning detection with diverse proposals",
    "Semantic parsing on Freebase from question-answer pairs",
    "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
    "Language models are few-shot learners",
    "Fast greedy map inference for determinantal point process to improve recommendation diversity",
    "Evaluating large language models trained on code",
    "BERT: Pre-training of deep bidirectional transformers for language understanding",
    "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources",
    "Human-level play in the game of diplomacy by combining language models with strategic reasoning",
    "Improving text-to-SQL evaluation methodology",
    "The pile: An 800gb dataset of diverse text for language modeling",
    "Simcse: Simple contrastive learning of sentence embeddings",
    "Near-optimal map inference for determinantal point processes",
    "Diverse sequential subset selection for supervised video summarization",
    "Faster greedy map inference for determinantal point processes",
    "Question decomposition with dependency graphs",
    "Momentum contrast for unsupervised visual representation learning",
    "ActivityNet: A large-scale video benchmark for human activity understanding",
    "The curious case of neural text degeneration",
    "Towards unsupervised dense information retrieval with contrastive learning",
    "Dense passage retrieval for open-domain question answering",
    "Adam: A method for stochastic optimization",
    "An exact algorithm for maximum entropy sampling",
    "k-dpps: Fixed-size determinantal point processes",
    "Determinantal point processes for machine learning",
    "Metric learning: A survey",
    "Diverse demonstrations improve in-context compositional generalization",
    "Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark",
    "On the advance of making language models better reasoners",
    "NL2Bash: A corpus and semantic parser for natural language interface to the linux operating system",
    "What makes good in-context examples for GPT-3?",
    "Learning to rank for information retrieval",
    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
    "WebGPT: Browser-assisted question-answering with human feedback",
    "Representation learning with contrastive predictive coding",
    "ChatGPT: Optimizing language models for dialogue",
    "Improving compositional generalization with latent structure and data augmentation",
    "Evaluating the impact of model scale for compositional generalization in semantic parsing",
    "Language models are unsupervised multitask learners",
    "The probabilistic relevance framework: BM25 and beyond",
    "Movie Description",
    "Learning to retrieve prompts for in-context learning",
    "Learning with kernels: support vector machines, regularization, optimization, and beyond",
    "Compositional generalization and natural language variation: Can a semantic parsing approach handle both?",
    "Natural language to code translation with execution",
    "Recursive deep models for semantic compositionality over a sentiment treebank",
    "Selective annotation makes language models better few-shot learners",
    "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
    "Attention is all you need",
    "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
    "Self-consistency improves chain of thought reasoning in language models",
    "Emergent abilities of large language models",
    "A broad-coverage challenge corpus for sentence understanding through inference",
    "Transformers: State-of-the-art natural language processing",
    "Break it down: A question understanding benchmark",
    "Self-adaptive in-context learning",
    "Deep determinantal point process for large-scale multi-label classification",
    "ProGen: Progressive zero-shot dataset generation via in-context feedback",
    "Generating data for symbolic language with large language models",
    "Complementary explanations for effective in-context learning",
    "Compositional generalization for neural semantic parsing via span-level supervised attention",
    "Extractive summarization as text matching"
]
}