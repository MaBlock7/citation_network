literature_titles = {
    "COMPOSITIONAL SEMANTIC PARSING WITH LARGE LANGUAGE MODELS": [
        "Do as I can, not as I say: Grounding language in robotic affordances",
        "Lexicon learning for few shot sequence modeling",
        "Learning to recombine and resample data for compositional generalization",
        "Good-enough compositional data augmentation",
        "CLOSURE: Assessing systematic generalization of CLEVR models",
        "Language models are few-shot learners",
        "Extracting training data from large language models",
        "Compositional generalization via neural-symbolic stack machines",
        "Syntactic structures",
        "Palm: Scaling language modeling with pathways",
        "Meta-learning to compositionally generalize",
        "The compositionality papers",
        "Compositional generalization in semantic parsing: Pre-training vs. specialized architectures",
        "Grounded graph decoding improves compositional generalization in question answering",
        "Measuring and improving compositional generalization in text-to-SQL via component alignment",
        "Permutation equivariant models for compositional generalization in language",
        "Hierarchical poset decoding for compositional generalization in language",
        "Span-based semantic parsing for compositional generalization",
        "Unlocking compositional generalization in pre-trained models using intermediate representations",
        "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning",
        "Measuring compositional generalization: A comprehensive method on realistic data",
        "COGS: A compositional generalization challenge based on semantic interpretation",
        "Sequence-to-sequence learning with latent neural grammars",
        "Thieves on sesame street! Model extraction of bert-based apis",
        "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
        "Compositional generalization through meta sequence-to-sequence learning",
        "Building machines that learn and think like people",
        "On the advance of making language models better reasoners",
        "Compositional generalization for primitive substitutions",
        "Compositional generalization by learning analytical expressions",
        "Rearranging the familiar: Testing compositional generalization in recurrent networks",
        "Universal grammar",
        "Compositional generalization in image captioning",
        "Learning compositional rules via neural program synthesis",
        "Training language models to follow instructions with human feedback",
        "Improving compositional generalization with latent structure and data augmentation",
        "Evaluating the impact of model scale for compositional generalization in semantic parsing",
        "A benchmark for systematic generalization in grounded language understanding",
        "Compositional generalization in a deep seq2seq model by separating syntax and semantics",
        "Compositional generalization and natural language variation: Can a semantic parsing approach handle both?",
        "Natural language to code translation with execution",
        "An introduction to unification-based approaches to grammar",
        "Few-shot semantic parsing with language models trained on code",
        "Constrained language models yield few-shot semantic parsers",
        "Self-consistency improves chain of thought reasoning in language models",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Learning synchronous grammars for semantic parsing with lambda calculus",
        "SEQZERO: Few-shot compositional semantic parsing with sequential prompts and zero-shot models",
        "Compositional generalization for neural semantic parsing via span-level supervised attention",
        "STaR: Bootstrapping reasoning with reasoning",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}