literature_titles = {
    "Language Models are Few-Shot Learners": [
        "Learning to learn by gradient descent by gradient descent",
        "Massively multilingual neural machine translation",
        "Language (technology) is power: A critical survey of “bias” in nlp",
        "Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining",
        "Experience grounds language",
        "Estimating or propagating gradients through stochastic neurons for conditional computation",
        "Multitask learning",
        "Think you have solved question answering? try arc, the ai2 reasoning challenge",
        "Generating long sequences with sparse transformers",
        "Uniter: Learning universal image-text representations",
        "The trouble with bias",
        "BERT: Pretraining of deep bidirectional transformers for language understanding",
        "Universal transformers",
        "Edinburgh's phrase-based machine translation systems for wmt-14",
        "Semi-supervised sequence learning",
        "Rl2: Fast reinforcement learning via slow reinforcement learning",
        "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
        "Transformer-xl: Attentive language models beyond a fixed-length context",
        "Understanding backtranslation at scale",
        "Model-agnostic meta-learning for fast adaptation of deep networks",
        "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
        "Adaptive computation time for recurrent neural networks",
        "Annotation artifacts in natural language inference data",
        "Gltr: Statistical detection and visualization of generated text",
        "Meta-learning for low-resource neural machine translation",
        "Ai and efficiency",
        "Deep learning scaling is predictable, empirically",
        "Distilling the knowledge in a neural network",
        "Learning to Learn Using Gradient Descent",
        "Reducing sentiment bias in language models via counterfactual evaluation",
        "Automatic detection of generated text is easiest when humans are fooled",
        "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
        "Numeric transformer - albert",
        "TinyBERT: Distilling BERT for natural language understanding",
        "Technical report on conversational question answering",
        "Unifiedqa: Crossing format boundaries with a single qa system",
        "All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation",
        "Scaling laws for neural language models",
        "Natural questions: a benchmark for question answering research",
        "Sequence-level knowledge distillation",
        "Nltk: The natural language toolkit",
        "Cross-lingual language model pretraining",
        "ALBERT: A lite BERT for self-supervised learning of language representations",
        "Adversarial training for large neural language models",
        "SummAE: Zero-shot abstractive text summarization using length-agnostic auto-encoders",
        "Story ending prediction by transferable bert",
        "Multilingual denoising pre-training for neural machine translation",
        "Representation learning using multi-task deep neural networks for semantic classification and information retrieval",
        "Improving multi-task deep neural networks via knowledge distillation for natural language understanding",
        "Multi-task deep neural networks for natural language understanding",
        "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
        "Learning to optimize neural nets",
        "RoBERTa: A robustly optimized BERT pretraining approach",
        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
        "Train large, then compress: Rethinking model size for efficient training and inference of transformers",
        "A corpus and evaluation framework for deeper understanding of commonsense stories",
        "An empirical model of large-batch training",
        "The penn treebank: annotating predicate argument structure",
        "The natural language decathlon: Multitask learning as question answering",
        "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
        "Model cards for model reporting",
        "Stereoset: Measuring stereotypical bias in pretrained language models",
        "Probing neural network comprehension of natural language arguments",
        "Fair is better than sensational: Man is to doctor as woman is to doctor",
        "Fascha",
        "Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks",
        "The lambada dataset: Word prediction requiring a broad discourse context",
        "A call for clarity in reporting BLEU scores",
        "Reducing gender bias in word-level language models with a gender-equalizing loss function",
        "Coqa: A conversational question answering challenge",
        "Few-shot autoregressive density estimation: Towards learning to learn distributions",
        "Optimization as a model for few-shot learning",
        "NumNet: Machine reading comprehension with numerical reasoning",
        "Gender bias in coreference resolution",
        "Guide for conducting risk assessments",
        "A constructive prediction of the generalization error across scales",
        "How much knowledge can you pack into the parameters of a language model?",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Language models are unsupervised multitask learners",
        "Release strategies and the social impacts of language models",
        "The woman worked as a babysitter: On biases in language generation",
        "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
        "Green AI",
        "Improving neural machine translation models with monolingual data",
        "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
        "Megatron-lm: Training multi-billion parameter language models using model parallelism",
        "Exploiting cloze questions for few-shot text classification and natural language inference",
        "MASS: Masked sequence to sequence pre-training for language generation",
        "Matching Networks for One Shot Learning",
        "Attention is all you need",
        "Superglue: A stickier benchmark for general-purpose language understanding systems",
        "Multi-agent dual learning",
        "Unsupervised data augmentation for consistency training",
        "XLNet: Generalized autoregressive pretraining for language understanding",
        "Hellaswag: Can a machine really finish your sentence?",
        "Defending against neural fake news",
        "Fine-tuning language models from human preferences"
    ]
}