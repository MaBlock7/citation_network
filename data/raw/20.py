literature_titles = {
    "Complementary Explanations for Effective In-Context Learning": [
        "Explanations for CommonsenseQA: New Dataset and Models",
        "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
        "Language models are few-shot learners",
        "e-snli: Natural language inference with natural language explanations",
        "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries",
        "Evaluating large language models trained on code",
        "Meta-learning via language model in-context tuning",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "Prototypical calibration for few-shot learning of language models",
        "Surface form competition: Why the highest probability answer isn't always right",
        "Maieutic prompting: Logically consistent reasoning with recursive explanations",
        "Large language models are zero-shot reasoners",
        "What makes good in-context examples for gpt-3?",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Language models of code are few-shot commonsense learners",
        "Noisy channel language model prompting for few-shot text classification",
        "MetaICL: Learning to learn in context",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Training language models to follow instructions with human feedback",
        "Measuring and narrowing the compositionality gap in language models",
        "Evaluating the impact of model scale for compositional generalization in semantic parsing",
        "Learning to retrieve prompts for in-context learning",
        "Constrained language models yield few-shot semantic parsers",
        "Selective annotation makes language models better few-shot learners",
        "Rationale-augmented ensembles in language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "An explanation of in-context learning as implicit Bayesian inference",
        "The unreliability of explanations in few-shot prompting for textual reasoning",
        "OPT: Open Pre-trained Transformer Language Models",
        "BERTScore: Evaluating Text Generation with BERT",
        "Calibrate before use: Improving few-shot performance of language models",
        "Least-to-most prompting enables complex reasoning in large language models",
        "Teaching algorithmic reasoning via in-context learning"
    ]
}
