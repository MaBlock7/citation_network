literature_titles = {
    "Language Models are Unsupervised Multitask Learners": [
        "Character-level language modeling with deeper self-attention",
        "A bert baseline for the natural questions",
        "Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects",
        "Deep speech 2: End-to-end speech recognition in english and mandarin",
        "Unsupervised neural machine translation",
        "An effective approach to unsupervised machine translation",
        "Layer normalization",
        "Embracing data abundance: Booktest dataset for reading comprehension",
        "Do we train on test data? purging cifar of near-duplicates",
        "A neural probabilistic language model",
        "Looking for elmo's friends: Sentence-level pretraining beyond language modeling",
        "Multitask learning",
        "One billion word benchmark for measuring progress in statistical language modeling",
        "Natural language processing (almost) from scratch",
        "Supervised learning of universal sentence representations from natural language inference data",
        "Word translation without parallel data",
        "Semi-supervised sequence learning",
        "Transformer-xl: Attentive language models beyond a fixed-length context",
        "The 14 billion word iweb corpus",
        "Universal transformers",
        "Bert: Pretraining of deep bidirectional transformers for language understanding",
        "Wizard of wikipedia: Knowledge-powered conversational agents",
        "Hierarchical neural story generation",
        "Model-agnostic metalearning for fast adaptation of deep networks",
        "Bottom-up abstractive summarization",
        "Multilingual language processing from bytes",
        "Frage: frequency-agnostic word representation",
        "Improving neural language models with a continuous cache",
        "Identity mappings in deep residual networks",
        "Deep learning scaling is predictable, empirically",
        "The goldilocks principle: Reading children's books with explicit memory representations",
        "Learning distributed representations of sentences from unlabelled data",
        "Entity tracking improves cloze-style reading comprehension",
        "Universal language model fine-tuning for text classification",
        "Interpolated estimation of markov source parameters from sparse data",
        "Adversarial examples for evaluating reading comprehension systems",
        "Exploring the limits of language modeling",
        "One model to learn them all",
        "Visualizing and understanding recurrent networks",
        "Overcoming catastrophic forgetting in neural networks",
        "Skip-thought vectors",
        "Imagenet classification with deep convolutional neural networks",
        "Natural questions: a benchmark for question answering research",
        "Building machines that learn and think like people",
        "Unsupervised machine translation using monolingual corpora only",
        "The winograd schema challenge",
        "Neural word embedding as implicit matrix factorization",
        "Generating wikipedia by summarizing long sequences",
        "Learned in translation: Contextualized word vectors",
        "The natural language decathlon: Multitask learning as question answering",
        "Pointer sentinel mixture models",
        "Distributed representations of words and phrases and their compositionality",
        "Abstractive text summarization using sequence-to-sequence rnns and beyond",
        "The lambada dataset: Word prediction requiring a broad discourse context",
        "Glove: Global vectors for word representation",
        "Content extraction using diverse feature sets",
        "Deep contextualized word representations",
        "Learning to generate reviews and discovering sentiment",
        "Improving language understanding by generative pre-training",
        "Unsupervised pretraining for sequence to sequence learning",
        "Do cifar-10 classifiers generalize to cifar-10?",
        "Coqa: A conversational question answering challenge",
        "Story cloze task: Uw nlp system",
        "Get to the point: Summarization with pointer-generator networks",
        "Neural machine translation of rare words with subword units",
        "Learning general purpose distributed sentence representations via large scale multi-task learning",
        "Sequence to sequence learning with neural networks",
        "Towards principled unsupervised learning",
        "On the evaluation of common-sense reasoning in natural language understanding",
        "A simple method for commonsense reasoning",
        "Attention is all you need",
        "A neural conversational model",
        "Pointer networks",
        "Glue: A multi-task benchmark and analysis platform for natural language understanding",
        "Dialog-based language learning",
        "No training required: Exploring random encoders for sentence classification",
        "Transfertransfo: A transfer learning approach for neural network based conversational agents",
        "Learning and evaluating general linguistic intelligence"
    ]
}