literature_titles = {
    "Making Large Language Models Better Reasoners with Step-Aware Verifier": [
        "Giving BERT a calculator: Finding operations and arguments with reading comprehension",
        "Logic-guided data augmentation and regularization for consistent question answering",
        "Commonsense for generative multi-hop question answering tasks",
        "Abductive commonsense reasoning",
        "Language models are few-shot learners",
        "Zero-shot transfer learning with synthesized data for multidomain dialogue state tracking",
        "HybridQA: A dataset of multi-hop question answering over tabular and textual data",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "Selection-inference: Exploiting large language models for interpretable logical reasoning",
        "ReasonBERT: Pretrained to reason with distant supervision",
        "Cognitive graph for multi-hop reading comprehension at scale",
        "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
        "Scalable multi-hop relational reasoning for knowledge-aware question answering",
        "Injecting numerical reasoning skills into language models",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Towards a unified view of parameter-efficient transfer learning",
        "Deberta: Decoding-enhanced bert with disentangled attention",
        "Surface form competition: Why the highest probability answer isn't always right",
        "Parameter-efficient transfer learning for nlp",
        "Lora: Low-rank adaptation of large language models",
        "A multi-type multi-span network for reading comprehension that requires discrete reasoning",
        "A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models",
        "Large language models are zero-shot reasoners",
        "Parsing algebraic word problems into equations",
        "Exploiting explicit paths for multi-hop reading comprehension",
        "Can language models learn from explanations in context?",
        "How many data points is a prompt worth?",
        "KagNet: Knowledge-aware graph networks for commonsense reasoning",
        "What makes good in-context examples for GPT3?",
        "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning",
        "Roberta: A robustly optimized bert pretraining approach",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "A diverse corpus for evaluating and developing english math word problem solvers",
        "Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge",
        "Metaicl: Learning to learn in context",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Are nlp models really able to solve simple math word problems?",
        "Reasoning like program executors",
        "Language models are unsupervised multitask learners",
        "Solving general arithmetic word problems",
        "Learning to retrieve prompts for in-context learning",
        "Generate & rank: A multi-task framework for math word problems",
        "Clutrr: A diagnostic benchmark for inductive reasoning from text",
        "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
        "Logic-driven context extension and data augmentation for logical reasoning of text",
        "Improving natural language inference using external knowledge in the science questions domain",
        "Multi-level recommendation reasoning over knowledge graphs with reinforcement learning",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Human parity on commonsenseqa: Augmenting self-attention with external attention",
        "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
        "Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills",
        "Reclor: A reading comprehension dataset requiring logical reasoning",
        "Star: Bootstrapping reasoning with reasoning",
        "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
        "Calibrate before use: Improving few-shot performance of language models",
        "Least-to-most prompting enables complex reasoning in large language models",
        "Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance"
    ]
}