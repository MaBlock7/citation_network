literature_titles = {
    "Learning To Retrieve Prompts for In-Context Learning": [
        "Neuro-symbolic language modeling with automaton-augmented retrieval",
        "Task-oriented dialogue as dataflow synthesis",
        "Optics: Ordering points to identify the clustering structure",
        "On the dangers of stochastic parrots: Can language models be too big?",
        "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
        "Improving language models by retrieving from trillions of tokens",
        "Language models are few-shot learners",
        "Reading Wikipedia to answer open-domain questions",
        "Evaluating large language models trained on code",
        "Case-based reasoning for natural language queries over knowledge bases",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "GLaM: Efficient scaling of language models with mixture-of-experts",
        "The pile: An 800gb dataset of diverse text for language modeling",
        "Retrieval augmented language model pre-training",
        "Question decomposition with dependency graphs",
        "BERTese: Learning to speak to BERT",
        "Efficient natural language response suggestion for smart reply",
        "Stochastic neighbor embedding",
        "Surface form competition: Why the highest probability answer isn't always right",
        "Billion-scale similarity search with GPUs",
        "Dense passage retrieval for open-domain question answering",
        "Nearest Neighbor Machine Translation",
        "Generalization through memorization: Nearest neighbor language models",
        "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
        "Adam: A method for stochastic optimization",
        "Latent retrieval for weakly supervised open domain question answering",
        "The inductive bias of in-context learning: Rethinking pretraining example design",
        "Retrieval-augmented generation for knowledge-intensive NLP tasks",
        "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "Jurassic-1: Technical details and evaluation",
        "What makes good in-context examples for gpt-3?",
        "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Noisy channel language model prompting for few-shot text classification",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Controllable semantic parsing via retrieval augmentation",
        "Language models as knowledge bases?",
        "Learning how to ask: Querying LMs with mixtures of soft prompts",
        "RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
        "The probabilistic relevance framework: BM25 and beyond",
        "Improving evidence retrieval for automated explainable fact-checking",
        "A mathematical exploration of why language models help solve downstream tasks",
        "Exploiting cloze-questions for few-shot text classification and natural language inference",
        "Improving the cluster structure extracted from OPTICS plots",
        "Constrained language models yield few-shot semantic parsers",
        "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
        "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
        "Training data is more valuable than you think: A simple and effective method by retrieving from training data",
        "Least-to-most prompting enables complex reasoning in large language models",
        "Large language models are human-level prompt engineers",
        "An explanation of in-context learning as implicit Bayesian inference",
        "Human parity on CommonsenseQA: Augmenting self-attention with external attention",
        "Calibrate before use: Improving few-shot performance of language models",
        "Factual probing is [MASK]: Learning vs. learning to recall"
    ]
}