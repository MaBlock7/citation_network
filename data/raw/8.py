literature_titles = {
    "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data": [
        "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
        "Square attack: a query-efficient black-box adversarial attack via random search",
        "PADA: Example-based prompt learning for on-the-fly adaptation to unseen domains",
        "Language models are few-shot learners",
        "e-snli: Natural language inference with natural language explanations",
        "Evaluating large language models trained on code",
        "Improving black-box adversarial attacks with a transfer-based prior",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "Commonsense knowledge mining from pretrained models",
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "Zen: Pre-training chinese text encoder enhanced by n-gram representations",
        "Black-box prompt learning for pre-trained language models",
        "Taming pre-trained language models with n-gram representations for low-resource domain adaptation",
        "Disarm: An antithetic gradient estimator for binary latent variables",
        "Complexity-based prompting for multi-step reasoning",
        "Normsage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly",
        "Making pre-trained language models better few-shot learners",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Retrieval-augmented gpt-3.5-based text-to-sql framework with sample-aware prompting and dynamic revision chain",
        "WARP: Word-level Adversarial ReProgramming",
        "PTR: Prompt Tuning with Rules for Text Classification",
        "BERTese: Learning to speak to BERT",
        "DEBERTA: Decoding-enhanced bert with disentangled attention",
        "Large language models can self-improve",
        "Black-box adversarial attack with transferable model-based embedding",
        "Black-box adversarial attacks with limited queries and information",
        "Prior convictions: Black-box adversarial attacks with bandits and priors",
        "How can we know what language models know?",
        "Large language models are zero-shot reasoners",
        "MAWPS: A math word problem repository",
        "Why machine reading comprehension models learn shortcuts?",
        "Can language models learn from explanations in context?",
        "Mwptoolkit: an open-source framework for deep learning-based math word problem solvers",
        "The power of scale for parameter-efficient prompt tuning",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "What makes good in-context examples for GPT-3?",
        "GPT Understands, Too",
        "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "Decoupled weight decay regularization",
        "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "A diverse corpus for evaluating and developing English math word problem solvers",
        "Can a suit of armor conduct electricity? A new dataset for open book question answering",
        "Training language models to follow instructions with human feedback",
        "The effect of prompting to students with different learning styles",
        "Reasoning like program executors",
        "Grips: Gradient-free, edit-based instruction search for prompting large language models",
        "Learning how to ask: Querying LMs with mixtures of soft prompts",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "The Probabilistic Relevance Framework: BM25 and Beyond",
        "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Selective annotation makes language models better few-shot learners",
        "What makes reading comprehension questions easier?",
        "Black-box tuning for language-model-as-a-service",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Universal adversarial triggers for attacking and analyzing NLP",
        "Rationale-augmented ensembles in language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Emergent abilities of large language models",
        "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
        "Human parity on commonsenseqa: Augmenting self-attention with external attention",
        "Adept: A debiasing prompt framework",
        "BARTScore: Evaluating generated text as text generation",
        "Star: Bootstrapping reasoning with reasoning",
        "Active example selection for in-context learning",
        "Automatic chain of thought prompting in large language models",
        "Calibrate before use: Improving few-shot performance of language models",
        "Factual probing is [MASK]: Learning vs. learning to recall",
        "Efficient neural network training via forward and backward propagation sparsification"
    ]
}