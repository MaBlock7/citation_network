literature_titles = {
    "Finding Support Examples for In-Context Learning": [
        "A survey on data-efficient algorithms in big data era",
        "On the dangers of stochastic parrots: Can language models be too big?",
        "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
        "Language models are few-shot learners",
        "Data curation alone can stabilize in-context learning",
        "On the relation between sensitivity and accuracy in in-context learning",
        "Super-samples from kernel herding",
        "Improving contrastive learning of sentence embeddings from AI feedback",
        "Selection via proxy: Efficient data selection for deep learning",
        "Support-vector networks",
        "V12. 1: User's manual for cplex",
        "Case-based reasoning for natural language queries over knowledge bases",
        "BERT: pre-training of deep bidirectional transformers for language understanding",
        "A survey for in-context learning",
        "The turking test: Can language models understand instructions?",
        "Zerogen+: Self-guided high-quality data generation in efficient zero-shot learning",
        "Deepcore: A comprehensive library for coreset selection in deep learning",
        "In-context learning for few-shot dialogue state tracking",
        "Submodular optimization with submodular cover and submodular knapsack constraints",
        "Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition, 2nd Edition",
        "GRAD-MATCH: gradient matching based data subset selection for efficient deep model training",
        "GLISTER: generalization based data subset selection for efficient and robust learning",
        "Reordering examples helps during priming-based few-shot learning",
        "Dbpedia A large-scale, multilingual knowledge base extracted from wikipedia",
        "Diverse demonstrations improve in-context compositional generalization",
        "What makes good in-context examples for gpt-3?",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Active learning by acquiring contrastive examples",
        "Hidden factors and hidden topics: understanding rating dimensions with review text",
        "Tuning language models as training data generators for augmentation-enhanced few-shot learning",
        "Noisy channel language model prompting for few-shot text classification",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Coresets for data-efficient training of machine learning models",
        "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
        "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
        "Deep learning on a data diet: Finding important examples early in training",
        "Language models are unsupervised multitask learners",
        "Sentence-bert: Sentence embeddings using siamese bert-networks",
        "A survey of deep active learning",
        "Learning to retrieve prompts for in-context learning",
        "Active learning for convolutional neural networks: A core-set approach",
        "XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
        "Core-set sampling for efficient neural architecture search",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Selective annotation makes language models better few-shot learners",
        "An empirical study of example forgetting during deep neural network learning",
        "Attention is all you need",
        "Building a question answering test collection",
        "Self-adaptive in-context learning",
        "Progen: Progressive zero-shot dataset generation via in-context feedback",
        "Compositional exemplars for in-context learning",
        "OPT: open pre-trained transformer language models",
        "Character-level convolutional networks for text classification",
        "Active example selection for in-context learning",
        "A survey of large language models",
        "Calibrate before use: Improving few-shot performance of language models"
    ]
}