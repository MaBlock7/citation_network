literature_titles = {
    "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels": [
        "Word Sense Induction with Neural biLM and Symmetric Patterns",
        "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
        "COMET : Commonsense Transformers for Automatic Knowledge Graph Construction",
        "Inducing Relational Knowledge from BERT",
        "Class-based n-gram models of natural language",
        "Language models are few-shot learners",
        "Boolq: Exploring the surprising difficulty of natural yes/no questions",
        "Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
        "PPT: Pre-trained Prompt Tuning for Few-shot Learning",
        "Earlystopped neural networks are consistent",
        "A mutual information maximization perspective of language representation learning",
        "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "Linguistic knowledge and transferability of contextual representations",
        "Pretrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
        "GPT Understands, Too",
        "Fantastically Ordered Prompts and Where to Find Them: Overcoming FewShot Prompt Order Sensitivity",
        "Learning word vectors for sentiment analysis",
        "A corpus and evaluation framework for deeper understanding of commonsense stories",
        "Distributional generalization: A new kind of generalization",
        "The LAMBADA dataset: Word prediction requiring a broad discourse context",
        "True Few-Shot Learning with Language Models",
        "Language models as knowledge bases?",
        "Wic: 10,000 example pairs for evaluating context-sensitive representations",
        "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "Know what you don't know: Unanswerable questions for squad",
        "Prompt programming for large language models: Beyond the few-shot paradigm",
        "Mutual information maximization for simple and accurate part-of-speech induction",
        "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
        "Information-Theoretic Probing with Minimum Description Length",
        "GPTJ-6B: A 6 Billion Parameter Autoregressive Language Model",
        "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners",
        "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
        "Representation Learning of Knowledge Graphs with Entity Attributes and Multimedia Descriptions"
    ]
}