literature_titles = {
    "LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS": [
        "Deep reinforcement learning at the edge of the statistical precipice",
        "Do as i can, not as i say: Grounding language in robotic affordances",
        "A general language assistant as a laboratory for alignment",
        "Program synthesis with large language models",
        "Efficient training of language models to fill in the middle",
        "PADA: A prompt-based autoregressive approach for adaptation to unseen domains",
        "Thinking aloud: Dynamic context generation improves zero-shot reasoning performance of GPT-2",
        "Language models are few-shot learners",
        "Evaluating large language models trained on code",
        "Training verifiers to solve math word problems",
        "Commonsense knowledge mining from pretrained models",
        "Neural program meta-induction",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "GLM: General language model pretraining with autoregressive blank infilling",
        "Learning libraries of subroutines for neurallyâ€“guided Bayesian program induction",
        "Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
        "Prompting: Better ways of using language models for nlp tasks",
        "Making pre-trained language models better few-shot learners",
        "Program synthesis",
        "Instruction induction: From few examples to natural language task descriptions",
        "Jigsaw: Large language models meet program synthesis",
        "How can we know what language models know?",
        "Scaling laws for neural language models",
        "Large language models are zero-shot reasoners",
        "Accelerating search-based program synthesis using learned probabilistic models",
        "The power of scale for parameter-efficient prompt tuning",
        "Competition-level code generation with alphacode",
        "Learning programs: A hierarchical Bayesian approach",
        "TruthfulQA: Measuring how models mimic human falsehoods",
        "GPT understands, too",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Firefly Monte Carlo: Exact MCMC with subsets of data",
        "A machine learning framework for programming by example",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Training language models to follow instructions with human feedback",
        "True few-shot learning with language models",
        "Learning how to ask: Querying LMs with mixtures of soft prompts",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Hierarchical text-conditional image generation with clip latents",
        "Prompt programming for large language models: Beyond the few-shot paradigm",
        "High-resolution image synthesis with latent diffusion models",
        "Solving general arithmetic word problems",
        "Multitask prompted training enables zero-shot task generalization",
        "Exploiting cloze-questions for few-shot text classification and natural language inference",
        "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Challenging BIG-bench tasks and whether chain-of-thought can solve them",
        "Attention is all you need",
        "Do prompt-based models really understand the meaning of their prompts?",
        "Finetuned language models are zero-shot learners",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Leveraging language to learn program abstractions and search heuristics",
        "BARTScore: Evaluating generated text as text generation",
        "STAR: Bootstrapping reasoning with reasoning",
        "GLM-130B: An open bilingual pre-trained model",
        "OPT: Open pre-trained transformer language models",
        "Fine-tuning language models from human preferences"
    ]
}