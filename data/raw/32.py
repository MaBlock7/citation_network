literature_titles = {
    "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference": [
        "Snowball: Extracting relations from large plain-text collections",
        "Semi-supervised bootstrapping of relationship extractors with distributional semantics",
        "Inducing relational knowledge from BERT",
        "Extracting patterns and relations from the world wide web",
        "MixText: Linguistically-informed interpolation of hidden space for semi-supervised text classification",
        "An embarrassingly simple approach for transfer learning from pretrained language models",
        "Unsupervised cross-lingual representation learning at scale",
        "Commonsense knowledge mining from pretrained models",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "Investigating meta-learning algorithms for low-resource natural language understanding tasks",
        "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
        "Meta-learning for low-resource neural machine translation",
        "On calibration of modern neural networks",
        "Revisiting self-training for neural sequence generation",
        "Distilling the knowledge in a neural network",
        "Iterative backtranslation for neural machine translation",
        "Universal language model fine-tuning for text classification",
        "Self-training PCFG grammars with latent annotations across languages",
        "How can we know what language models know?",
        "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
        "RoBERTa: A robustly optimized BERT pretraining approach",
        "The natural language decathlon: Multitask learning as question answering",
        "Effective self-training for parsing",
        "Argumentative relation classification as plausibility ranking",
        "Automatic differentiation in PyTorch",
        "Language models as knowledge bases?",
        "Zero-shot text classification with generative language models",
        "Domain adaptive dialog generation via meta learning",
        "Improving language understanding by generative pre-training",
        "Language models are unsupervised multitask learners",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets",
        "An embarrassingly simple approach to zero-shot learning",
        "WinoGrande: An adversarial winograd schema challenge at scale",
        "Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking",
        "Improving neural machine translation models with monolingual data",
        "Zero-shot learning of classifiers from natural language quantification",
        "How to fine-tune BERT for text classification?",
        "oLMpics â€“ on what language model pre-training captures",
        "A simple method for commonsense reasoning",
        "X-stance: A multilingual multi-target dataset for stance detection",
        "Improving language understanding by generative pre-training",
        "Unsupervised data augmentation for consistency training",
        "XLNet: Generalized autoregressive pretraining for language understanding",
        "Unsupervised word sense disambiguation rivaling supervised methods",
        "Zero-shot text classification: Datasets, evaluation and entailment approach",
        "Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach",
        "Diverse few-shot text classification with multiple metrics",
        "Character-level convolutional networks for text classification"
    ]
}