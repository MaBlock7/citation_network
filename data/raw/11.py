literature_titles = {
    "Batch Prompting: Efficient Inference with Large Language Model APIs": [
        "In-context examples selection for machine translation",
        "Ask me anything: A simple strategy for prompting language models",
        "PromptSource: An integrated development environment and repository for natural language prompts",
        "The fifth pascal recognizing textual entailment challenge",
        "Language models are few-shot learners",
        "Evaluating large language models trained on code",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Binding language models in symbolic languages",
        "PaLM: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "PAL: Program-aided language models",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Learning to solve arithmetic word problems with verb categorization",
        "How can we know what language models know?",
        "Non-autoregressive machine translation with disentangled context transformer",
        "Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation",
        "Finetuning pretrained transformers into RNNs",
        "Transformers are RNNs: Fast autoregressive transformers with linear attention",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "What makes good in-context examples for GPT-3?",
        "Training language models to follow instructions with human feedback",
        "Compositional semantic parsing on semi-structured tables",
        "Are NLP models really able to solve simple math word problems?",
        "ABC: Attention with bounded-memory control",
        "Random feature attention",
        "Solving general arithmetic word problems",
        "Learning to retrieve prompts for in-context learning",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Selective annotation makes language models better few-shot learners",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Model cascading: Towards jointly improving efficiency and accuracy of NLP systems",
        "Attention is all you need",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "A broad-coverage challenge corpus for sentence understanding through inference",
        "React: Synergizing reasoning and acting in language models",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}