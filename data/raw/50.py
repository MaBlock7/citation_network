literature_titles = {
    "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs": [
        "Have llms advanced enough? a challenging problem solving benchmark for large language models",
        "Program synthesis with large language models",
        "Adaptive importance sampling to accelerate training of a neural probabilistic language model",
        "An adaptive sampling scheme to efficiently train fully convolutional networks for semantic segmentation",
        "Optimal testing for crowd workers",
        "Language models are few-shot learners",
        "Codet: Code generation with generated tests",
        "Frugalgpt: How to use large language models while reducing cost and improving performance",
        "Evaluating large language models trained on code",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "Training verifiers to solve math word problems",
        "Pomdp-based control of workflows for crowdsourcing",
        "Maximum likelihood estimation of observer error-rates using the em algorithm",
        "Crowdsourcing systems on the world-wide web",
        "Reducing transformer depth on demand with structured dropout",
        "Pal: Program-aided language models",
        "Romebert: Robust training of multiexit bert",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Measuring coding challenge competence with apps",
        "Dynabert: Dynamic bert with adaptive width and depth",
        "Top-kast: Top-k always sparse training",
        "Large Language Models are Zero-Shot Reasoners",
        "Solving quantitative reasoning problems with language models",
        "Competition-level code generation with alphacode",
        "Crowdsourcing control: Moving beyond multiple choice",
        "Fastbert: a self-distilling bert with adaptive inference time",
        "Automix: Automatically mixing language models",
        "Learning performance-improving code edits",
        "Flowgen: Fast and slow graph generation",
        "A diverse corpus for evaluating and developing English math word problem solvers",
        "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
        "Revisiting prompt engineering via declarative crowdsourcing",
        "Are nlp models really able to solve simple math word problems?",
        "Sampling bias in deep active classification: An empirical study",
        "Human computation: a survey and taxonomy of a growing field",
        "Learning from crowds",
        "Confident adaptive language modeling",
        "Consistent accelerated inference via confident adaptive transformers",
        "Datasets for Studying Generalization from Easy to Hard Examples",
        "Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks",
        "Stanford alpaca: An instruction-following llama model",
        "Llama: Open and efficient foundation language models",
        "RationaleAugmented Ensembles in Language Models",
        "Selfconsistency improves chain of thought reasoning in language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Artificial intelligence and collective intelligence",
        "The multidimensional wisdom of crowds",
        "Hyperparameter estimation in dirichlet process mixture models",
        "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise",
        "Deebert: Dynamic early exiting for accelerating bert inference",
        "Early exit or not: Resource-efficient blind quality enhancement for compressed images",
        "Adaptive computation with elastic input sequence",
        "Bert loses patience: Fast and robust inference with early exit"
    ]
}