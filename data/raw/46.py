literature_titles = {
    "LARGE LANGUAGE MODELS AS ANALOGICAL REASONERS": [
        "Palm 2 technical report",
        "Program synthesis with large language models",
        "Knowledge-powered deep learning for word embedding",
        "From machine learning to machine reasoning: An essay",
        "Language models are few-shot learners",
        "Large language models as tool makers",
        "Learning by analogy: Formulating and generalizing plans from past experience",
        "Evaluating large language models trained on code",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Execution-guided neural program synthesis",
        "Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension",
        "Teaching large language models to self-debug",
        "Binding language models in symbolic languages",
        "Palm: Scaling language modeling with pathways",
        "Scaling instruction-finetuned language models",
        "Training verifiers to solve math word problems",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "Compositional semantic parsing with large language models",
        "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
        "The analogical paradox: Why analogy is so easy in naturalistic settings yet so difficult in the psychological laboratory",
        "Scalable multihop relational reasoning for knowledge-aware question answering",
        "The pile: An 800gb dataset of diverse text for language modeling",
        "Structure-mapping: A theoretical framework for analogy",
        "Reasoning and learning by analogy: Introduction",
        "Structure mapping in analogy and similarity",
        "Synthesize, execute and debug: Learning to repair for neural program synthesis",
        "Reasoning with language model is planning with world model",
        "Exploring human-like translation strategy with large language models",
        "Measuring coding challenge competence with apps",
        "Measuring mathematical problem solving with the math dataset",
        "Analogy and relational reasoning",
        "In-context analogical reasoning with pre-trained language models",
        "A diagnostic study of visual question answering with analogical reasoning",
        "How can we know what language models know?",
        "Maieutic prompting: Logically consistent reasoning with recursive explanations",
        "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "Language models can solve computer tasks",
        "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
        "Large language models are zero-shot reasoners",
        "Spoc: Search-based pseudocode to code",
        "Self-prompting large language models for open-domain qa",
        "Competition-level code generation with alphacode",
        "Holistic evaluation of language models",
        "Improving mathematical reasoning with process supervision",
        "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
        "What makes good in-context examples for gpt-3?",
        "Data contamination: From memorization to exploitation",
        "Cross-task generalization via natural language crowdsourcing instructions",
        "Analogical reasoning in the context of acquiring problem solving expertise",
        "Med-flamingo: a multimodal medical few-shot learner",
        "Gpt-4 technical report",
        "Training language models to follow instructions with human feedback",
        "Generative agents: Interactive simulacra of human behavior",
        "How to solve it: A new aspect of mathematical method",
        "Measuring and narrowing the compositionality gap in language models",
        "Is chatgpt a general-purpose natural language processing task solver?",
        "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
        "Lego: Latent execution-guided reasoning for multi-hop question answering on knowledge graphs",
        "Toolformer: Language models can teach themselves to use tools",
        "Replug: Retrieval-augmented black-box language models",
        "Automatic prompt augmentation and selection with chain-of-thought from labeled data",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Selective annotation makes language models better few-shot learners",
        "Recitation-augmented language models",
        "Challenging big-bench tasks and whether chain-of-thought can solve them",
        "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
        "Similarity and analogical reasoning",
        "Self-consistency improves chain of thought reasoning in language models",
        "Creative thought: An investigation of conceptual structures and processes",
        "Emergent analogical reasoning in large language models",
        "Finetuned language models are zero-shot learners",
        "Chain of thought prompting elicits reasoning in large language models",
        "Lime: Learning inductive bias for primitives of mathematical reasoning",
        "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
        "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
        "React: Synergizing reasoning and acting in language models",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "Graph-based, self-supervised program repair from diagnostic feedback",
        "Break-it-fix-it: Unsupervised learning for program repair",
        "QA-GNN: Reasoning with language models and knowledge graphs for question answering",
        "Deep bidirectional language-knowledge graph pretraining",
        "LinkBERT: Pretraining language models with document links",
        "Retrieval-augmented multimodal language modeling",
        "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task",
        "Parsel: A unified natural language framework for algorithmic reasoning",
        "Improved logical reasoning of language models via differentiable symbolic programming",
        "Greaselm: Graph reasoning enhanced language models for question answering",
        "Automatic chain of thought prompting in large language models",
        "Complex reasoning in natural language",
        "Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}