literature_titles = {
    "kNN PROMPTING: BEYOND-CONTEXT LEARNING WITH CALIBRATION-FREE NEAREST NEIGHBOR INFERENCE": [
        "Language (technology) is power: A critical survey of “bias” in NLP",
        "On the opportunities and risks of foundation models",
        "Improving language models by retrieving from trillions of tokens",
        "Language models are few-shot learners",
        "Decoupling knowledge from memorization: Retrieval-augmented prompt learning",
        "Meta-learning via language model in-context tuning",
        "Palm: Scaling language modeling with pathways",
        "Lamda: Language models for dialog applications",
        "The pascal recognising textual entailment challenge",
        "Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers",
        "The commitmentbank: Investigating projection in naturally occurring discourse",
        "A survey for in-context learning",
        "Discriminatory analysis. nonparametric discrimination: Consistency properties",
        "SimCSE: Simple contrastive learning of sentence embeddings",
        "Mitigating gender bias in distilled language models via counterfactual role reversal",
        "Generating sentences by editing prototypes",
        "Measuring massive multitask language understanding",
        "Deep learning scaling is predictable, empirically",
        "Surface form competition: Why the highest probability answer isn't always right",
        "Mining and summarizing customer reviews",
        "Few-shot learning with retrieval augmented language models",
        "How can we know what language models know?",
        "How can we know when language models know? on the calibration of language models for question answering",
        "Learning to remember rare events",
        "Generalization through memorization: Nearest neighbor language models",
        "Nearest neighbor machine translation",
        "Large language models are zero-shot reasoners",
        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
        "Trans-encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations",
        "What makes good in-context examples for GPT-3?",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Noisy channel language model prompting for few-shot text classification",
        "MetaICL: Learning to learn in context",
        "StereoSet: Measuring stereotypical bias in pretrained language models",
        "A simple cache model for image recognition",
        "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
        "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
        "Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning",
        "Language models are unsupervised multitask learners",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
        "A constructive prediction of the generalization error across scales",
        "Learning to retrieve prompts for in-context learning",
        "Multitask prompted training enables zero-shot task generalization",
        "Exploiting cloze-questions for few-shot text classification and natural language inference",
        "Nearest neighbor zero-shot inference",
        "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "A simple method for commonsense reasoning",
        "Improvements to bm25 and language models examined",
        "Visualizing data using t-sne",
        "Building a question answering test collection",
        "Training data is more valuable than you think: A simple and effective method by retrieving from training data",
        "Finetuned language models are zero-shot learners",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Retrieve and refine: Improved sequence generation models for dialogue",
        "Annotating expressions of opinions and emotions in language",
        "An explanation of in-context learning as implicit bayesian inference",
        "Opt: Open pre-trained transformer language models",
        "Character-level convolutional networks for text classification",
        "Calibrate before use: Improving few-shot performance of language models"
    ]
}