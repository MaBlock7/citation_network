literature_titles = {
    "How Can We Know What Language Models Know?": [
        "Snowball: Extracting relations from large plaintext collections",
        "A neural knowledge language model",
        "Matching the blanks: Distributional similarity for relation learning",
        "Open information extraction from the web",
        "What do neural machine translation models learn about morphology?",
        "Analysis methods in neural language processing: A survey",
        "Large scale acquisition of paraphrases for learning surface patterns",
        "Inducing relational knowledge from BERT",
        "A survey of automatic query expansion in information retrieval",
        "Semi-supervised sequence learning",
        "BERT: Pretraining of deep bidirectional transformers for language understanding",
        "HotFlip: White-box adversarial examples for text classification",
        "T-REx: A large scale alignment of natural language with knowledge base triples",
        "Identifying relations for open information extraction",
        "Sentence-level MT evaluation without reference translations: Beyond language modeling",
        "Mask-predict: Parallel decoding of conditional masked language models",
        "Assessing BERT's syntactic abilities",
        "Latent relation language models",
        "A structural probe for finding syntax in word representations",
        "Towards decoding as continuous optimisation in neural machine translation",
        "Barack's wife Hillary: Using knowledge graphs for fact-aware language modeling",
        "What does BERT learn about the structure of language?",
        "Adam: A method for stochastic optimization",
        "A diversity-promoting objective function for neural conversation models",
        "Understanding neural networks through representation erasure",
        "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
        "Paraphrasing revisited with neural machine translation",
        "The natural language decathlon: Multitask learning as question answering",
        "context2vec: Learning generic context embedding with bidirectional LSTM",
        "On the state of the art of evaluation in neural language models",
        "Regularizing and optimizing LSTM language models",
        "Context dependent recurrent neural network language model",
        "Facebook FAIR's WMT19 news translation task submission",
        "Deep contextualized word representations",
        "Knowledge enhanced contextual word representations",
        "Language models as knowledge bases?",
        "BERT is not a knowledge base (yet): Factual knowledge vs. Name-based reasoning in unsupervised QA",
        "Language models are unsupervised multitask learners",
        "Explain yourself! Leveraging language models for commonsense reasoning",
        "Learning surface text patterns for a question answering system",
        "Investigating a generic paraphrase-based approach for relation extraction",
        "Atomic: An atlas of machine commonsense for if-then reasoning",
        "Improving neural machine translation models with monolingual data",
        "Does string-based neural MT learn source syntax?",
        "What do recurrent neural network grammars learn about syntax?",
        "BERT rediscovers the classical NLP pipeline",
        "What do you learn from context? Probing for sentence structure in contextualized word representations",
        "Representing text for joint embedding of text and knowledge bases",
        "A simple method for commonsense reasoning",
        "Universal adversarial triggers for attacking and analyzing NLP",
        "Reference-aware language models",
        "ERNIE: Enhanced language representation with informative entities",
        "The Microsoft Research sentence completion challenge"
    ]
}