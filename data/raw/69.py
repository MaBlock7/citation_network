literature_titles = {
    "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks": [
        "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
        "Modeling of the question answering task in the yodaqa system",
        "Semantic Parsing on Freebase from Question-Answer Pairs",
        "Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation",
        "Reading Wikipedia to Answer Open-Domain Questions",
        "Coarse-to-fine question answering for long documents",
        "Simple and Effective Multi-Paragraph Reading Comprehension",
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "Wizard of wikipedia: Knowledge-powered conversational agents",
        "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine",
        "Hierarchical neural story generation",
        "ELI5: Long form question answering",
        "Augmenting transformers with KNN-based composite memory",
        "Entities as experts: Sparse memory access with entity supervision",
        "A knowledge-grounded neural conversation model",
        "When will AI exceed human performance? evidence from AI experts",
        "Search engine guided neural machine translation",
        "Generating sentences by editing prototypes",
        "REALM: Retrieval-augmented language model pre-training",
        "A retrieve-and-edit framework for predicting structured outputs",
        "Simple and effective retrieve-edit-rerank text generation",
        "Billion-scale similarity search with gpus",
        "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "Inferring algorithmic patterns with stackaugmented recurrent nets",
        "Dense passage retrieval for open-domain question answering",
        "Generalization through memorization: Nearest neighbor language models",
        "Adam: A method for stochastic optimization",
        "Natural Questions: a Benchmark for Question Answering Research",
        "Large memory layers with product keys",
        "Latent retrieval for weakly supervised open domain question answering",
        "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
        "A diversity-promoting objective function for neural conversation models",
        "Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons",
        "Robust neural machine translation with joint textual and phonetic embedding",
        "Generating wikipedia by summarizing long sequences",
        "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs",
        "The next decade in ai: four steps towards robust artificial intelligence",
        "How decoding strategies affect the verifiability of generated text",
        "Mixed precision training",
        "Towards exploiting background knowledge for building conversation systems",
        "Towards a better metric for evaluating question generation systems",
        "MS MARCO: A human generated machine reading comprehension dataset",
        "Passage re-ranking with BERT",
        "fairseq: A fast, extensible toolkit for sequence modeling",
        "Finding generalizable evidence by learning to convince q&a models",
        "Language models as knowledge bases?",
        "How context affects language models' factual predictions",
        "Improving Language Understanding by Generative Pre-Training",
        "Language models are unsupervised multitask learners",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "How much knowledge can you pack into the parameters of a language model?",
        "The probabilistic relevance framework: Bm25 and beyond",
        "Release strategies and the social impacts of language models",
        "End-to-end memory networks",
        "FEVER: a large-scale dataset for fact extraction and VERification",
        "Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification with elastic weight consolidation",
        "Attention is all you need",
        "Diverse beam search for improved description of complex scenes",
        "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
        "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
        "R3: Reinforced ranker-reader for open-domain question answering",
        "Evidence aggregation for answer reranking in open-domain question answering",
        "Memory networks",
        "Retrieve and refine: Improved sequence generation models for dialogue",
        "Huggingface's transformers: State-of-the-art natural language processing",
        "Addressing semantic drift in question generation for semi-supervised question answering",
        "Reasoning over semantic-level graph for fact checking"
    ]
}