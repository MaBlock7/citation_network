literature_titles = {
    "SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions": [
        "Self-training: A survey",
        "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
        "Language models are few-shot learners",
        "Scaling instruction-finetuned language models",
        "Self-training improves pre-training for natural language understanding",
        "A survey of data augmentation approaches for nlp",
        "Speaker-follower models for vision-and-language navigation",
        "Revisiting self-training for neural sequence generation",
        "Distilling the knowledge in a neural network",
        "Unnatural instructions: Tuning language models with (almost) no human labor",
        "Instruction induction: From few examples to natural language task descriptions",
        "Large language models can self-improve",
        "Large language models struggle to learn long-tail knowledge",
        "Multilingual constituency parsing with self-attention and pre-training",
        "Constituency parsing with a self-attentive encoder",
        "The power of scale for parameter-efficient prompt tuning",
        "WANLI: Worker and ai collaboration for natural language inference dataset creation",
        "Teaching small language models to reason",
        "Leveraging qa datasets to improve generative data augmentation",
        "Tuning language models as training data generators for augmentation-enhanced few-shot learning",
        "FILM: Following Instructions in Language with Modular Methods",
        "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
        "Training Language Models to Follow Instructions with Human Feedback",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Impact of pretraining term frequencies on few-shot reasoning",
        "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
        "Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "Generating datasets with pretrained language models",
        "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
        "Explaining patterns in data with language models via interpretable autoprompting",
        "Principle-driven selfalignment of language models from scratch with minimal human supervision",
        "Stanford alpaca: An instruction-following llama model",
        "Super-naturalinstructions: Generalization via declarative instructions on 1600+ tasks",
        "Towards zero-label language learning",
        "Finetuned Language Models are Zero-Shot Learners",
        "One-Shot Learning from a Demonstration with Hierarchical Latent Language",
        "Generating sequences by learning to selfcorrect",
        "Learning from Task Descriptions",
        "Symbolic knowledge distillation: from general language models to commonsense models",
        "Self-training with noisy student improves imagenet classification",
        "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
        "Generative data augmentation for commonsense reasoning",
        "Guess the instruction! making language models stronger zero-shot learners",
        "STar: Self-taught reasoner bootstrapping reasoning with reasoning",
        "Pre-trained language models can be fully zero-shot learners",
        "Prompt Consistency for Zero-Shot Task Generalization",
        "Large language models are human-level prompt engineers"
    ]
}