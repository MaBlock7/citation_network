literature_titles = {
    "Deductive Verification of Chain-of-Thought Reasoning": [
        "Why exposure bias matters: An imitation learning perspective of error accumulation in language generation",
        "Natural language deduction through search over statement compositions",
        "Language models are few-shot learners",
        "Sparks of artificial general intelligence: Early experiments with gpt-4",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Teaching large language models to self-debug",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "PaLM: Scaling language modeling with pathways",
        "Scaling instruction-finetuned language models",
        "Training verifiers to solve math word problems",
        "Faithful reasoning using large language models",
        "Selection-inference: Exploiting large language models for interpretable logical reasoning",
        "Palm-e: An embodied multimodal language model",
        "Compositional semantic parsing with large language models",
        "Roscoe: A suite of metrics for scoring step-by-step reasoning",
        "Hallucinations in large multilingual translation models",
        "Measuring mathematical problem solving with the math dataset",
        "Training compute-optimal large language models",
        "Learning to solve arithmetic word problems with verb categorization",
        "Survey of hallucination in natural language generation",
        "Large language models are zero-shot reasoners",
        "Learning to automatically solve algebra word problems",
        "Can language models learn from explanations in context?",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Learn to explain: Multimodal reasoning via thought chains for science question answering",
        "Faithful chain-of-thought reasoning",
        "Self-refine: Iterative refinement with self-feedback",
        "Few-shot self-rationalization with natural language prompts",
        "On faithfulness and factuality in abstractive summarization",
        "Gpt-4 technical report",
        "Training language models to follow instructions with human feedback",
        "Refiner: Reasoning feedback on intermediate representations",
        "Receval: Evaluating reasoning chains via correctness and informativeness",
        "Street: A multi-task structured reasoning and explanation benchmark",
        "Solving general arithmetic word problems",
        "Multitask prompted training enables zero-shot task generalization",
        "Bloom: A 176b-parameter open-access multilingual language model",
        "Toolformer: Language models can teach themselves to use tools",
        "Generate & rank: A multi-task framework for math word problems",
        "Large language models can be easily distracted by irrelevant context",
        "Language models are multilingual chain-of-thought reasoners",
        "Reflexion: an autonomous agent with dynamic memory and self-reflection",
        "Prompting gpt-3 to be reliable",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
        "Llama: Open and efficient foundation language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Neural text generation with unlikelihood training",
        "Large language models are reasoners with self-verification",
        "Generating natural language proofs with verifier-guided search",
        "React: Synergizing reasoning and acting in language models",
        "Star: Self-taught reasoner bootstrapping reasoning with reasoning",
        "Socratic models: Composing zero-shot multimodal reasoning with language",
        "Opt: Open pre-trained transformer language models",
        "Automatic chain of thought prompting in large language models",
        "Least-to-most prompting enables complex reasoning in large language models",
        "Teaching algorithmic reasoning via in-context learning"
    ]
}