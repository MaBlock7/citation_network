literature_titles = {
    "COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING": [
        "Notes on teaching gpt-3 adding numbers",
        "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
        "Language models are few-shot learners",
        "Evaluating large language models trained on code",
        "Learning an Executable Neural Semantic Parser",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "Selection-inference: Exploiting large language models for interpretable logical reasoning",
        "Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies",
        "Scaling laws for neural language models",
        "Large language models are zero-shot reasoners",
        "Why machine reading comprehension models learn shortcuts?",
        "Internet-augmented language models through few-shot prompting for open-domain question answering",
        "Solving quantitative reasoning problems with language models",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "On the advance of making language models better reasoners",
        "Learning executable semantic parsers for natural language understanding",
        "What makes good in-context examples for GPT-3?",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Did the model understand the question?",
        "Sentence-bert: Sentence embeddings using siamese bert-networks",
        "Pushing the limits of rule reasoning in transformers through natural language satisfiability",
        "Extrapolating to unnatural language processing with gpt-3's in-context learning: The good, the bad, and the mysterious",
        "Solving general arithmetic word problems",
        "Learning to retrieve prompts for in-context learning",
        "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
        "Selective annotation makes language models better few-shot learners",
        "What makes reading comprehension questions easier?",
        "Challenging big-bench tasks and whether chain-of-thought can solve them",
        "LaMDA: Language models for dialog applications",
        "Universal adversarial triggers for attacking and analyzing NLP",
        "Rationale-augmented ensembles in language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Why do pretrained language models help in downstream tasks? An analysis of head and prompt tuning",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "An explanation of in-context learning as implicit Bayesian inference",
        "StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing",
        "Calibrate before use: Improving few-shot performance of language models"
    ]
}