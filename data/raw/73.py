literature_titles = {
    "Self-Evaluation Guided Beam Search for Reasoning": [
        "A learning algorithm for boltzmann machines",
        "Language models are few-shot learners",
        "Language gans falling short",
        "Evaluating large language models trained on code",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "Hierarchical neural story generation",
        "PAL: program-aided language models",
        "Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies",
        "Sequence transduction with recurrent neural networks",
        "On calibration of modern neural networks",
        "Long text generation via adversarial training with leaked information",
        "The curious case of neural text degeneration",
        "Large language models can self-improve",
        "How can we know When language models know? on the calibration of language models for question answering",
        "Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition",
        "Language models (mostly) know what they know",
        "Large language models are zero-shot reasoners",
        "Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement",
        "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
        "Solving quantitative reasoning problems with language models",
        "On the advance of making language models better reasoners",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
        "Self-refine: Iterative refinement with self-feedback",
        "Conditional poisson stochastic beam search",
        "Best-first beam search",
        "A diverse corpus for evaluating and developing english math word problem solvers",
        "Show your work: Scratchpads for intermediate computation with language models",
        "GPT-4 technical report",
        "Are NLP models really able to solve simple math word problems?",
        "REFINER: reasoning feedback on intermediate representations",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "Reflexion: an autonomous agent with dynamic memory and self-reflection",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "On NMT search errors and model errors: Cat got your tongue?",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Llama: Open and efficient foundation language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Solving math word problems with process- and outcome-based feedback",
        "Shepherd pre-trained language models to develop a train of thought: An iterative prompting approach",
        "Self-consistency improves chain of thought reasoning in language models",
        "Consistency of a recurrent language model with respect to incomplete decoding",
        "Exploiting reasoning chains for multi-hop science question answering",
        "Human parity on commonsenseQA: Augmenting self-attention with external attention",
        "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
        "The unreliability of explanations in few-shot prompting for textual reasoning",
        "Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts",
        "Calibrate before use: Improving few-shot performance of language models",
        "Least-to-most prompting enables complex reasoning in large language models",
        "Towards interpretable natural language understanding with explanations as latent variables"
    ]
}