literature_titles = {
    "LogiCoT: Logical Chain-of-Thought Instruction Tuning": [
        "Falcon-40B: an open large language model with state-of-the-art performance",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "Scaling instruction-finetuned language models",
        "Explaining answers with entailment trees",
        "Specializing smaller language models towards multi-step reasoning",
        "Pal: Program-aided language models",
        "Folio: Natural language reasoning with first-order logic",
        "Measuring mathematical problem solving with the math dataset",
        "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
        "Towards reasoning in large language models: A survey",
        "Large language models are zero-shot reasoners",
        "Symbolic chain-of-thought distillation: Small models can also 'think' step-by-step",
        "Augmenting neural networks with first-order logic",
        "Evaluating the logical reasoning ability of chatgpt and gpt-4",
        "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning",
        "From zero to hero: Examining the power of symbolic tasks in instruction tuning",
        "The flan collection: Designing data and methods for effective instruction tuning",
        "Natural logic for textual inference",
        "Teaching small language models to reason",
        "Cross-task generalization via natural language crowdsourcing instructions",
        "Logicinference: A new dataset for teaching logical inference to seq2seq models",
        "Gpt-4 technical report",
        "Training language models to follow instructions with human feedback",
        "Instruction tuning with gpt-4",
        "Multitask prompted training enables zero-shot task generalization",
        "Paradigm shift in natural language processing",
        "Stanford alpaca: An instruction-following llama model",
        "Llama: Open and efficient foundation language models",
        "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
        "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
        "Pinto: Faithful language reasoning using prompt-generated rationales",
        "Self-instruct: Aligning language model with self generated instructions",
        "Finetuned language models are zero-shot learners",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Reclor: A reading comprehension dataset requiring logical reasoning",
        "Star: Bootstrapping reasoning with reasoning",
        "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
        "Large language models are human-level prompt engineers"
    ]
}