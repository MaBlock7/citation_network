literature_titles = {
    "Better Zero-Shot Reasoning with Self-Adaptive Prompting": [
        "Language models are few-shot learners",
        "Ensemble selection from libraries of models",
        "Evaluating large language models trained on code",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "RLPrompt: Optimizing discrete text prompts with reinforcement learning",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Semi-supervised learning by entropy minimization",
        "Unnatural instructions: Tuning language models with (almost) no human labor",
        "Learning to solve arithmetic word problems with verb categorization",
        "Large language models can self-improve",
        "OpenNMT: Open-source toolkit for neural machine translation",
        "Large language models are zero-shot reasoners",
        "Parsing algebraic word problems into equations",
        "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
        "Self-prompting large language models for open-domain qa",
        "On the advance of making language models better reasoners",
        "What makes good in-context examples for GPT-3?",
        "Text and patterns: For effective chain of thought, it takes two to tango",
        "Sentence-t5: Scalable sentence encoders from pretrained text-to-text models",
        "A deep reinforced model for abstractive summarization",
        "Language models are unsupervised multitask learners",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
        "In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning",
        "Solving general arithmetic word problems",
        "Learning to retrieve prompts for in-context learning",
        "ChatGPT: Optimizing language models for dialogue",
        "Transductive semi-supervised deep learning using min-max features",
        "Selective annotation makes language models better few-shot learners",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Lamda: Language models for dialog applications",
        "Learning values across many orders of magnitude",
        "Superglue: A stickier benchmark for general-purpose language understanding systems",
        "Self-consistency improves chain of thought reasoning in language models",
        "Self-instruct: Aligning language model with self generated instructions",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Ethical and social risks of harm from language models",
        "Large language models are reasoners with self-verification",
        "Zero-shot learning-the good, the bad and the ugly",
        "React: Synergizing reasoning and acting in language models",
        "Star: Self-taught reasoner bootstrapping reasoning with reasoning",
        "Automatic chain of thought prompting in large language models"
    ]
}