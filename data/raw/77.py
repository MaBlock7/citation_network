literature_titles = {
    "SKELETON-OF-THOUGHT: PROMPTING LLMS FOR EFFICIENT PARALLEL GENERATION": [
        "Introducing claude",
        "Graph of thoughts: Solving elaborate problems with large language models",
        "Language models are few-shot learners",
        "Once-for-all: Train one network and specialize it for efficient deployment",
        "LangChain",
        "Accelerating large language model decoding with speculative sampling",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Dynamic n: M fine-grained structured sparse attention mechanism",
        "LLM zoo: democratizing chatgpt",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "Scaling instruction-finetuned language models",
        "Flashattention: Fast and memory-efficient exact attention with io-awareness",
        "Exploiting linear structure within convolutional networks for efficient evaluation",
        "Enhancing chat language models by scaling high-quality instructional conversations",
        "GLM: General language model pretraining with autoregressive blank infilling",
        "Neural architecture search: A survey",
        "Hierarchical neural story generation",
        "Turbotransformers: an efficient gpu serving system for transformer models",
        "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
        "GPTQ: Accurate post-training quantization for generative pre-trained transformers",
        "Compressing large-scale transformer-based models: A case study on bert",
        "Assisted generation: a new direction toward low-latency text generation",
        "Tensorflow serving",
        "Non-autoregressive neural machine translation",
        "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
        "Data-centric ai",
        "Large language models can self-improve",
        "GPipe: Efficient training of giant neural networks using pipeline parallelism",
        "Data movement is all you need: A case study on optimizing transformers",
        "LLMLingua: Compressing prompts for accelerated inference of large language models",
        "Reformer: The efficient transformer",
        "Quantizing deep convolutional networks for efficient inference: A whitepaper",
        "One weird trick for parallelizing convolutional neural networks",
        "Efficient memory management for large language model serving with pagedattention",
        "GShard: Scaling giant models with conditional computation and automatic sharding",
        "The power of scale for parameter-efficient prompt tuning",
        "Fast inference from transformers via speculative decoding",
        "CAMEL: Communicative agents for 'mind' exploration of large scale language model society",
        "A hierarchical neural autoencoder for paragraphs and documents",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "ALPACAeval: An automatic evaluator of instruction-following models",
        "Making language models better reasoners with step-aware verifier",
        "Terapipe: Token-level pipeline parallelism for training large-scale language models",
        "AWQ: Activation-aware weight quantization for LLM compression and acceleration",
        "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Roberta: A robustly optimized bert pretraining approach",
        "Decoupled weight decay regularization",
        "FlexFlow: A flexible dataflow accelerator architecture for convolutional neural networks",
        "SpecInfer: Accelerating generative LLM serving with speculative inference and token tree verification",
        "Accelerating sparse deep neural networks",
        "PipeDream: Generalized pipeline parallelism for DNN training",
        "Memory-efficient pipeline-parallel DNN training",
        "FasterTransformer",
        "Triton inference server",
        "Training language models to follow instructions with human feedback",
        "StableVicuna-13b",
        "Measuring and narrowing the compositionality gap in language models",
        "Data-to-text generation with content selection and planning",
        "Zero: Memory optimizations toward training trillion parameter models",
        "ZeRO-Offload: Democratizing Billion-Scale model training",
        "Accelerating transformer inference for translation via parallel decoding",
        "Toolformer: Language models can teach themselves to use tools",
        "Lightllm",
        "Openppl",
        "Long and diverse text generation with planning-based hierarchical variational model",
        "Fast transformer decoding: One write-head is all you need",
        "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
        "High-throughput generative inference of large language models with a single gpu",
        "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
        "Blockwise parallel decoding for deep autoregressive models",
        "Spectr: Fast speculative decoding via optimal transport",
        "Rethinking the inception architecture for computer vision",
        "Alpaca: A strong, replicable instruction-following model",
        "Llama: Open and efficient foundation language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Openllms: Less is more for open-source models",
        "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
        "Linformer: Self-attention with linear complexity",
        "Self-consistency improves chain of thought reasoning in language models",
        "Dice semimetric losses: Optimizing the dice score with soft labels",
        "Finetuned language models are zero-shot learners",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Learning structured sparsity in deep neural networks",
        "Smoothquant: Accurate and efficient post-training quantization for large language models",
        "A survey on non-autoregressive generation for neural machine translation and beyond",
        "Wizardlm: Empowering large language models to follow complex instructions",
        "GSPMD: general and scalable parallelization for ml computation graphs",
        "React: Synergizing reasoning and acting in language models",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "Orca: A distributed serving system for Transformer-Based generative models",
        "Big bird: Transformers for longer sequences",
        "Star: Bootstrapping reasoning with reasoning",
        "Data-centric artificial intelligence: A survey",
        "Bytetransformer: A high-performance transformer boosted for variable-length inputs",
        "Cumulative reasoning with large language models",
        "Alpa: Automating inter-and IntraOperator parallelism for distributed deep learning",
        "Judging llm-as-a-judge with mt-bench and chatbot arena",
        "Lima: Less is more for alignment",
        "PetS: A unified framework for Parameter-Efficient transformers serving",
        "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
        "Neural architecture search with reinforcement learning"
    ]
}