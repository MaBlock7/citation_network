literature_titles = {
    "Demystifying Prompts in Language Models via Perplexity Estimation": [
        "Promptsource: An integrated development environment and repository for natural language prompts",
        "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification",
        "Language models are few-shot learners",
        "On the relation between sensitivity and accuracy in in-context learning",
        "No language left behind: Scaling human-centered machine translation",
        "Northeuralex: a wide-coverage lexical database of northern eurasia",
        "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
        "Measuring causal effects of data statistics on language model's 'factual' predictions",
        "Making pre-trained language models better few-shot learners",
        "Orca: Interpreting prompted language models via locating supporting data evidence in the ocean of pretraining data",
        "Surface form competition: Why the highest probability answer isn't always right",
        "Large language models struggle to learn long-tail knowledge",
        "Reframing instructional prompts to gptk's language",
        "Prompt waywardness: The curious case of discretized interpretation of continuous prompts",
        "How many data points is a prompt worth?",
        "Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia",
        "The power of scale for parameter-efficient prompt tuning",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "Estimating the carbon footprint of bloom, a 176b parameter language model",
        "Learning word vectors for sentiment analysis",
        "Wordnet: a lexical database for english",
        "Multi-source social feedback of online news feeds",
        "Learning how to ask: Querying lms with mixtures of soft prompts",
        "Impact of pretraining term frequencies on few-shot reasoning",
        "CARER: Contextualized affect representations for emotion recognition",
        "It's not just size that matters: Small language models are also few-shot learners",
        "Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?",
        "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
        "Neural network acceptability judgments",
        "Learning paraphrastic sentence embeddings from back-translated bitext",
        "Opt: Open pre-trained transformer language models",
        "Character-level convolutional networks for text classification"
    ]
}