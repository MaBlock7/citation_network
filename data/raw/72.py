literature_titles = {
    "SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS": [
        "A learning algorithm for boltzmann machines",
        "Towards a human-like open-domain chatbot",
        "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
        "Giving BERT a calculator: Finding operations and arguments with reading comprehension",
        "Learning to retrieve reasoning paths over wikipedia graph for question answering",
        "The second pascal recognising textual entailment challenge",
        "Diverse m-best solutions in markov random fields",
        "The fifth pascal recognizing textual entailment challenge",
        "Beyond the imitation game: Measuring and extrapolating the capabilities of language models",
        "Language models are few-shot learners",
        "esnli: Natural language inference with natural language explanations",
        "Make up your mind! adversarial generation of inconsistent natural language explanations",
        "Multi-hop question answering via reasoning chains",
        "Evaluating large language models trained on code",
        "Palm: Scaling language modeling with pathways",
        "Boolq: Exploring the surprising difficulty of natural yes/no questions",
        "Think you have solved question answering? try arc, the ai2 reasoning challenge",
        "Training verifiers to solve math word problems",
        "The pascal recognising textual entailment challenge",
        "Is MAP decoding all you need? The inadequacy of the mode in neural machine translation",
        "Measuring and improving consistency in pretrained language models",
        "Intuition and reasoning: A dual-process perspective",
        "Hierarchical neural story generation",
        "Controlling linguistic style aspects in neural language generation",
        "Making pre-trained language models better few-shot learners",
        "Injecting numerical reasoning skills into language models",
        "Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies",
        "The third pascal recognizing textual entailment challenge",
        "Learning to write with cooperative discriminators",
        "The curious case of neural text degeneration",
        "Learning to solve arithmetic word problems with verb categorization",
        "UNIFIEDQA: Crossing format boundaries with a single QA system",
        "Large language models are zero-shot reasoners",
        "MAWPS: A math word problem repository",
        "MWPToolkit: An open-source framework for deep learning-based math word problem solvers",
        "Mutual information and diverse decoding improve neural machine translation",
        "A simple, fast diverse decoding algorithm for neural generation",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Typical decoding for natural language generation",
        "A diverse corpus for evaluating and developing English math word problem solvers",
        "Adversarial NLI: A new benchmark for natural language understanding",
        "Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning",
        "Are NLP models really able to solve simple math word problems?",
        "Reasoning like program executors",
        "Measuring and improving BERT's mathematical abilities by predicting the order of reasoning",
        "Language models are unsupervised multitask learners",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "NumNet: Machine reading comprehension with numerical reasoning",
        "Solving general arithmetic word problems",
        "Generate & rank: A multi-task framework for math word problems",
        "Natural language to code translation with execution",
        "Individual differences in reasoning: Implications for the rationality debate?",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Unifying language learning paradigms",
        "Lamda: Language models for dialog applications",
        "Diverse beam search for improved description of complex scenes",
        "Chain of thought prompting elicits reasoning in large language models",
        "Consistency of a recurrent language model with respect to incomplete decoding",
        "Exploiting reasoning chains for multi-hop science question answering",
        "Human parity on commonsenseqa: Augmenting self-attention with external attention",
        "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
        "The unreliability of explanations in few-shot prompting for textual reasoning",
        "Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts",
        "Calibrate before use: Improving few-shot performance of language models"
    ]
}