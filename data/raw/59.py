literature_titles = {
    "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents": [
        "Language models are few-shot learners",
        "Long context question answering via supervised contrastive learning",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Scaling instruction-finetuned language models",
        "Training verifiers to solve math word problems",
        "A dataset of information-seeking questions and answers anchored in research papers",
        "Successive prompting for decomposing complex questions",
        "Pal: Program-aided language models",
        "Constructing inferences during narrative text comprehension",
        "Large language models can self-improve",
        "Towards reasoning in large language models: A survey",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "Language models can solve computer tasks",
        "The NarrativeQA reading comprehension challenge",
        "Hurdles to progress in long-form question answering",
        "Natural questions: A benchmark for question answering research",
        "Large language model guided tree-of-thought",
        "Chameleon: Plug-and-play compositional reasoning with large language models",
        "Self-refine: Iterative refinement with self-feedback",
        "Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning",
        "Gpt-4 technical report",
        "Training language models to follow instructions with human feedback",
        "QuALITY: Question answering with long input texts, yes!",
        "Measuring and narrowing the compositionality gap in language models",
        "Summarization programs: Interpretable abstractive summarization with neural modular trees",
        "Toolformer: Language models can teach themselves to use tools",
        "SCROLLS: Standardized CompaRison over long language sequences",
        "Reflexion: an autonomous agent with dynamic memory and self-reflection",
        "ASQA: Factoid questions meet long-form answers",
        "Self-checkgpt: Zero-resource black-box hallucination detection for generative large language models",
        "Augmented language models: a survey",
        "ConditionalQA: A complex reading comprehension dataset with conditional answers",
        "Iterative hierarchical attention for answering complex questions over long documents",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Llama: Open and efficient foundation language models",
        "Asking and answering questions to evaluate the factual consistency of summaries",
        "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "How do we answer complex questions: Discourse structure of long-form answers",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "React: Synergizing reasoning and acting in language models",
        "Answering questions by meta-reasoning over multiple chains of thought",
        "OPT: Open pretrained transformer language models",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}