literature_titles = {
    "Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts": [
        "Language models are few-shot learners",
        "Chatgpt: Optimizing language models for dialogue",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "Making pre-trained language models better few-shot learners",
        "Ppt: Pre-trained prompt tuning for few-shot learning",
        "Mining and summarizing customer reviews",
        "How can we know what language models know?",
        "Reframing instructional prompts to gptk's language",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "Roberta: A robustly optimized bert pretraining approach",
        "Wordnet: a lexical database for english",
        "Thumbs up? sentiment classification using machine learning techniques",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Exploiting cloze-questions for few-shot text classification and natural language inference",
        "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Transprompt: Towards an automatic transferable prompting framework for few-shot text classification",
        "Towards unified prompt tuning for few-shot text classification",
        "Discrete and soft prompting for multilingual models",
        "Adapting language models for zeroshot learning by meta-tuning on dataset and prompt collections",
        "Factual probing is [mask]: Learning vs. learning to recall"
    ]
}