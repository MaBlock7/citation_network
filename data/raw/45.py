literature_titles = {
    "Large Language Models are Zero-Shot Reasoners": [
        "Do as i can, not as i say: Grounding language in robotic affordances",
        "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
        "Language models are few-shot learners",
        "On the measure of intelligence",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "The pile: An 800gb dataset of diverse text for language modeling",
        "Making pre-trained language models better few-shot learners",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
        "Learning to solve arithmetic word problems with verb categorization",
        "The structure of human intelligence: It is verbal, perceptual, and image rotation (VPR), not fluid and crystallized",
        "Parsing algebraic word problems into equations",
        "MAWPS: A math word problem repository",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "What makes good in-context examples for gpt-3?",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "The Cattell-Horn-Carroll theory of cognitive abilities: Past, present, and future",
        "Pointer sentinel mixture models",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Training language models to follow instructions with human feedback",
        "True few-shot learning with language models",
        "Learning how to ask: Querying LMs with mixtures of soft prompts",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Explain yourself! leveraging language models for commonsense reasoning",
        "Prompt programming for large language models: Beyond the few-shot paradigm",
        "Solving general arithmetic word problems",
        "Multitask prompted training enables zero-shot task generalization",
        "It's not just size that matters: Small language models are also few-shot learners",
        "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
        "Unsupervised commonsense question answering with self-talk",
        "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Individual differences in reasoning: Implications for the rationality debate?",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Lamda: Language models for dialog applications",
        "Attention is all you need",
        "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
        "Self-consistency improves chain of thought reasoning in language models",
        "Do prompt-based models really understand the meaning of their prompts?",
        "Chain of thought prompting elicits reasoning in large language models",
        "Transformers: State-of-the-art natural language processing",
        "STAR: Bootstrapping reasoning with reasoning",
        "OPT: Open pre-trained transformer language models"
    ]
}