literature_titles = {
    "Unified Demonstration Retriever for In-Context Learning": [
        "In-context examples selection for machine translation",
        "Beyond opinion mining: Summarizing opinions of customer reviews",
        "Task-oriented dialogue as dataflow synthesis",
        "Benchmarking applied semantic inference: The PASCAL recognising textual entailment challenges",
        "On the dangers of stochastic parrots: Can language models be too big?",
        "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-TensorFlow",
        "A large annotated corpus for learning natural language inference",
        "Language models are few-shot learners",
        "From RankNet to LambdaRank to LambdaMART: An overview",
        "Evaluating large language models trained on code",
        "On the relation between sensitivity and accuracy in in-context learning",
        "Improving contrastive learning of sentence embeddings from AI feedback",
        "A discourse-aware attention model for abstractive summarization of long documents",
        "Cross-lingual language model pretraining",
        "Case-based reasoning for natural language queries over knowledge bases",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "A survey for in-context learning",
        "Semantic Noise Matters for Neural Natural Language Generation",
        "Deberta: Decoding-enhanced BERT with disentangled attention",
        "Teaching machines to read and comprehend",
        "In-context learning for few-shot dialogue state tracking",
        "Cosmos QA: Machine reading comprehension with contextual commonsense reasoning",
        "Neural CRF model for sentence alignment in text simplification",
        "Billion-scale similarity search with GPUs",
        "Dense passage retrieval for open-domain question answering",
        "Abstractive summarization of Reddit posts with multi-level memory networks",
        "Dbpedia: A large-scale, multilingual knowledge base extracted from Wikipedia",
        "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
        "Finding supporting examples for in-context learning",
        "Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts",
        "CommonGen: A constrained text generation challenge for generative commonsense reasoning",
        "What makes good in-context examples for gpt-3?",
        "Roberta: A robustly optimized BERT pretraining approach",
        "Decoupled weight decay regularization",
        "Codexglue: A machine learning benchmark dataset for code understanding and generation",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Hidden factors and hidden topics: understanding rating dimensions with review text",
        "Noisy channel language model prompting for few-shot text classification",
        "A corpus and cloze evaluation for deeper understanding of commonsense stories",
        "TSATC: Twitter Sentiment Analysis Training Corpus",
        "DART: Open-domain structured data record to text generation",
        "Training language models to follow instructions with human feedback",
        "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
        "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
        "Synchromesh: Reliable code generation from pre-trained language models",
        "Language models are unsupervised multitask learners",
        "SentenceBERT: Sentence embeddings using Siamese BERT-networks",
        "The probabilistic relevance framework: BM25 and beyond",
        "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
        "Learning to retrieve prompts for in-context learning",
        "XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-SQL semantic parsing",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "One embedder, any task: Instruction-finetuned text embeddings",
        "Attention is all you need",
        "Building a question answering test collection",
        "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
        "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
        "Does it make sense? And why? A pilot study for sense making and explanation",
        "A broad-coverage challenge corpus for sentence understanding through inference",
        "Break it down: A question understanding benchmark",
        "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
        "knn prompting: Beyond-context learning with calibration-free nearest neighbor inference",
        "Character-level convolutional networks for text classification",
        "Active example selection for in-context learning"
    ]
}