literature_titles = {
    "Thread of Thought Unraveling Chaotic Contexts": [
        "The reversal curse: Llms trained on \"a is b\" fail to learn \"b is a\"",
        "Graph of thoughts: Solving elaborate problems with large language models",
        "Language models are few-shot learners",
        "Extending context window of large language models via positional interpolation",
        "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
        "Longnet: Scaling transformers to 1, 000, 000, 000 tokens",
        "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression",
        "Retrieval-augmented generation for knowledge-intensive NLP tasks",
        "Lost in the middle: How language models use long contexts",
        "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
        "Adaptive machine translation with large language models",
        "GPT-4 technical report",
        "Train short, test long: Attention with linear biases enables input length extrapolation",
        "Parallel context windows for large language models",
        "Chatgpt: Optimizing language models for dialogue",
        "Simple entity-centric questions challenge dense retrievers",
        "Evaluating the factual consistency of large language models through news summarization",
        "Llama: Open and efficient foundation language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Chatcad: Interactive computer-aided diagnosis on medical image using large language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Efficient streaming language models with attention sinks",
        "Beyond goldfish memory: Long-term open-domain conversation",
        "Retrieval meets long context large language models",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "Beyond chain-of-thought, effective graph-of-thought reasoning in large language models",
        "Disc-lawllm: Fine-tuning large language models for intelligent legal services",
        "Sentiment analysis in the era of large language models: A reality check",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}