literature_titles = {
    "Tree of Thoughts: Deliberate Problem Solving with Large Language Models": [
        "Language models are few-shot learners",
        "A survey of monte carlo tree search methods",
        "Deep blue",
        "Teaching large language models to self-debug",
        "Palm: Scaling language modeling with pathways",
        "Faithful reasoning using large language models",
        "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control",
        "Pal: Program-aided language models",
        "Reasoning with language model is planning with world model",
        "A formal basis for the heuristic determination of minimum cost paths",
        "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
        "Inner monologue: Embodied reasoning through planning with language models",
        "Maieutic prompting: Logically consistent reasoning with recursive explanations",
        "Thinking, fast and slow",
        "Representativeness revisited: Attribute substitution in intuitive judgment",
        "Language models can solve computer tasks",
        "Llm+p: Empowering large language models with optimal planning proficiency",
        "Neurologic a*esque decoding: Constrained text generation with lookahead heuristics",
        "Self-refine: Iterative refinement with self-feedback",
        "Report on a general problem solving program",
        "Human problem solving",
        "Gpt-4 technical report",
        "Refiner: Reasoning feedback on intermediate representations",
        "Improving language understanding by generative pre-training",
        "Language models are unsupervised multitask learners",
        "Large language model programs",
        "Reflexion: an autonomous agent with dynamic memory and self-reflection",
        "Mastering the game of go without human knowledge",
        "The empirical case for two systems of reasoning",
        "Who is rational? Studies of individual differences in reasoning",
        "Llama: Open and efficient foundation language models",
        "Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning",
        "Automated crossword solving",
        "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
        "Chain of thought prompting elicits reasoning in large language models",
        "Decomposition enhances reasoning via self-evaluation guided decoding",
        "Foundation models for decision making: Problems, methods, and opportunities",
        "ReAct: Synergizing reasoning and acting in language models",
        "Planning with large language models for code generation",
        "Least-to-most prompting enables complex reasoning in large language models",
        "Solving math word problem via cooperative reasoning induced language models"
    ]
}