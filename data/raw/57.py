literature_titles = {
    "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve": [
        "Incontext examples selection for machine translation",
        "Palm 2 technical report",
        "On the dangers of stochastic parrots: Can language models be too big?",
        "Language models are few-shot learners for prognostic prediction",
        "Palm: Scaling language modeling with pathways",
        "Scaling instruction-finetuned language models",
        "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
        "Case-based reasoning for natural language queries over knowledge bases",
        "A theory of unconscious thought",
        "A survey for in-context learning",
        "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
        "Metacognition",
        "The role of consciousness in memory",
        "Specializing smaller language models towards multi-step reasoning",
        "Zerogen+: Self-guided high-quality data generation in efficient zero-shot learning",
        "Large language models are reasoning teachers",
        "Training compute-optimal large language models",
        "Unnatural instructions: Tuning language models with (almost) no human labor",
        "Incontext learning for few-shot dialogue state tracking",
        "Large language models can self-improve",
        "Towards reasoning in large language models: A survey",
        "Language models (mostly) know what they know",
        "Can language models learn from explanations in context?",
        "Diverse demonstrations improve in-context compositional generalization",
        "Self-prompting large language models for open-domain QA",
        "Unified demonstration retriever for incontext learning",
        "Finding supporting examples for in-context learning",
        "Generating with confidence: Uncertainty quantification for black-box large language models",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "What makes good in-context examples for gpt-3?",
        "Fastbert: a self-distilling BERT with adaptive inference time",
        "Large language model guided tree-of-thought",
        "Z-ICL: zero-shot in-context learning with pseudo-demonstrations",
        "Teaching small language models to reason",
        "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
        "Augmented language models: a survey",
        "Can a suit of armor conduct electricity? a new dataset for open book question answering",
        "Adversarial NLI: A new benchmark for natural language understanding",
        "GPT-4 technical report",
        "Training language models to follow instructions with human feedback",
        "Measuring and narrowing the compositionality gap in language models",
        "Pre-trained models for natural language processing: A survey",
        "Sentence-bert: Sentence embeddings using siamese bert-networks",
        "The probabilistic relevance framework: BM25 and beyond",
        "Learning to retrieve prompts for in-context learning",
        "Multitask prompted training enables zero-shot task generalization",
        "The cognitive neuroscience of constructive memory: Remembering the past and imagining the future",
        "Toolformer: Language models can teach themselves to use tools",
        "Synthetic prompting: Generating chain-of-thought demonstrations for large language models",
        "XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Selective annotation makes language models better few-shot learners",
        "One embedder, any task: Instruction-finetuned text embeddings",
        "Unifying language learning paradigms",
        "Simple Heuristics That Make Us Smart",
        "Llama: Open and efficient foundation language models",
        "Episodic memory: From mind to brain",
        "Does it make sense? and why? A pilot study for sense making and explanation",
        "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
        "Rationale-augmented ensembles in language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Self-instruct: Aligning language model with self generated instructions",
        "Finetuned language models are zero-shot learners",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Large language models are reasoners with self-verification",
        "Deebert: Dynamic early exiting for accelerating BERT inference",
        "Zerogen: Efficient zero-shot learning via dataset generation",
        "Progen: Progressive zero-shot dataset generation via in-context feedback",
        "Compositional exemplars for in-context learning",
        "Ground-truth labels matter: A deeper look into input-label demonstrations",
        "Star: Bootstrapping reasoning with reasoning",
        "OPT: open pre-trained transformer language models",
        "Automatic chain of thought prompting in large language models",
        "A survey of large language models",
        "Progressive-hint prompting improves reasoning in large language models",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}