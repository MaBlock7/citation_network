literature_titles = {
    "Post Hoc Explanations of Language Models Can Improve Language Models": [
        "On the opportunities and risks of foundation models",
        "Language models are few-shot learners",
        "Sparks of artificial general intelligence: Early experiments with gpt-4",
        "Interpretation of black box nlp models: A survey",
        "BERT: pre-training of deep bidirectional transformers for language understanding",
        "A survey for in-context learning",
        "Towards a rigorous science of interpretable machine learning",
        "Towards benchmarking the utility of explanations for model debugging",
        "How can i choose an explainer? an application-grounded evaluation of post-hoc explanations",
        "Survey of hallucination in natural language generation",
        "The disagreement problem in explainable machine learning: A practitioner's perspective",
        "Robust and stable black box explanations",
        "Can language models learn from explanations in context?",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Estimating the carbon footprint of bloom, a 176b parameter language model",
        "A unified approach to interpreting model predictions",
        "Post-hoc interpretability for neural nlp: A survey",
        "Training language models to follow instructions with human feedback",
        "Language models are unsupervised multitask learners",
        "Why should I trust you? Explaining the predictions of any classifier",
        "Learning important features through propagating activation differences",
        "Deep inside convolutional networks: Visualising image classification models and saliency maps",
        "Reliable post hoc explanations: Modeling uncertainty in explainability",
        "Talktomodel: Understanding machine learning models with open ended dialogues",
        "Smoothgrad: Removing noise by adding noise",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Axiomatic attribution for deep networks",
        "Challenging big-bench tasks and whether chain-of-thought can solve them",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
        "Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Interpreting language models with contrastive explanations",
        "Visualizing and understanding convolutional networks",
        "Agieval: A human-centric benchmark for evaluating foundation models"
    ]
}