literature_titles = {
    "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method": [
        "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
        "Language models are few-shot learners",
        "Gpt-3 and instructgpt: technological dystopianism, utopianism, and “contextual” perspectives in ai ethics and industry",
        "Unisumm: Unified few-shot summarization with multi-task pre-training and prefix-tuning",
        "Palm: Scaling language modeling with pathways",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
        "SummEval: Re-evaluating summarization evaluation",
        "Bottom-up abstractive summarization",
        "News summarization and evaluation in the era of gpt-3",
        "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
        "The effects of human variation in DUC summarization evaluation",
        "Ctrlsum: Towards generic controllable text summarization",
        "Teaching machines to read and comprehend",
        "Large language models are zero-shot reasoners",
        "Wikihow: A large scale text summarization dataset",
        "Neural text summarization: A critical evaluation",
        "The structure and function of communication in society",
        "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
        "Reader-aware multi-document summarization: An enhanced model and the first dataset",
        "A technique for the measurement of attitudes",
        "ROUGE: A package for automatic evaluation of summaries",
        "Fine-tune bert for extractive summarization",
        "Roberta: A robustly optimized bert pretraining approach",
        "Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation",
        "BRIO: Bringing order to abstractive summarization",
        "Explicitly modeling importance and coherence for timeline summarization",
        "On faithfulness and factuality in abstractive summarization",
        "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
        "Annotated Gigaword",
        "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
        "A well-composed text is half done! composition sampling for diverse conditional generation",
        "Planning with learned entity prompts for abstractive summarization",
        "Better summarization evaluation with word embeddings for ROUGE",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Training language models to follow instructions with human feedback",
        "Overview of the tac 2011 summarization track: Guided task and aesop task",
        "Bleu: a method for automatic evaluation of machine translation",
        "The inverted pyramid—when and why did it appear?",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "COMET: A neural framework for MT evaluation",
        "The new york times annotated corpus",
        "Multitask prompted training enables zero-shot task generalization",
        "BLEURT: Learning robust metrics for text generation",
        "Language models are multilingual chain-of-thought reasoners",
        "Learning to summarize with human feedback",
        "Sequence to sequence learning with neural networks",
        "Lamda: Language models for dialog applications",
        "Attention is all you need",
        "Pointer networks",
        "Self-consistency improves chain of thought reasoning in language models",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
        "Bertscore: Evaluating text generation with BERT",
        "Benchmarking large language models for news summarization",
        "HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization",
        "Automatic chain of thought prompting in large language models",
        "Multimodal chain-of-thought reasoning in language models",
        "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}