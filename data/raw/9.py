literature_titles = {
    "Automatic Prompt Optimization with “Gradient Descent” and Beam Search": [
        "Best arm identification in multi-armed bandits",
        "Nltk: the natural language toolkit",
        "Regret analysis of stochastic and nonstochastic multiarmed bandit problems",
        "Sparks of artificial general intelligence: Early experiments with gpt-4",
        "Teaching large language models to self-debug",
        "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
        "From arabic sentiment analysis to sarcasm detection: The arsarcasm dataset",
        "Making pre-trained language models better few-shot learners",
        "Learning to program with natural language",
        "Warp: Word-level adversarial reprogramming",
        "Optimizing prompts for text-to-image generation",
        "Instruction induction: From few examples to natural language task descriptions",
        "Promptmaker: Prompt-based prototyping with large language models",
        "Almost optimal exploration in multi-armed bandits",
        "Algorithms for multi-armed bandit problems",
        "The power of scale for parameter-efficient prompt tuning",
        "Competition-level code generation with alphacode",
        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
        "Ethos: an online hate speech detection dataset",
        "Gpt-4 technical report",
        "Grips: Gradient-free, edit-based instruction search for prompting large language models",
        "Learning how to ask: Querying lms with mixtures of soft prompts",
        "Prompt programming for large language models: Beyond the few-shot paradigm",
        "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
        "\"liar, liar pants on fire\": A new benchmark dataset for fake news detection",
        "Self-instruct: Aligning language model with self generated instructions",
        "Gps: Genetic prompt search for efficient few-shot learning",
        "Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts",
        "Socratic models: Composing zero-shot multimodal reasoning with language",
        "Tempera: Test-time prompt editing via reinforcement learning",
        "Large language models are human-level prompt engineers"
    ]
}