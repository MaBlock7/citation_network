literature_titles = {
    "CHAIN-OF-TABLE: EVOLVING TABLES IN THE REASONING CHAIN FOR TABLE UNDERSTANDING": [
        "Table-to-text generation and pre-training with TabT5",
        "Palm 2 technical report",
        "Language models are few-shot learners",
        "Webtables: Exploring the power of tables on the web",
        "Large language models are few(1)-shot table reasoners",
        "Tabfact: A large-scale dataset for table-based fact verification",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Binding language models in symbolic languages",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "Handling divergent reference texts when evaluating table-to-text generation",
        "Understanding tables with intermediate pre-training",
        "PAL: Program-aided language models",
        "PASTA: Table-operations aware fact verification via sentence-table cloze pre-training",
        "Reasoning with language model is planning with world model",
        "TaPas: Weakly supervised table parsing via pre-training",
        "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
        "MathPrompter: Mathematical reasoning using large language models",
        "OmniTab: Pretraining with natural and synthetic data for few-shot table-based question answering",
        "A survey on table question answering: recent advances",
        "Tab-cot: Zero-shot tabular chain of thought",
        "A survey on deep learning approaches for text-to-sql",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "ROUGE: A package for automatic evaluation of summaries",
        "Lost in the middle: How language models use long contexts",
        "TAPEX: Table pre-training via learning a neural sql executor",
        "From zero to hero: Examining the power of symbolic tasks in instruction tuning",
        "Benchmarking large language model capabilities for conditional generation",
        "FeTaQA: Free-form table question answering",
        "Lever: Learning to verify language-to-code generation with execution",
        "Bleu: a method for automatic evaluation of machine translation",
        "Compositional semantic parsing on semi-structured tables",
        "'favourite'sql-statementsâ€”an empirical analysis of sql-usage in commercial applications",
        "Evaluating the text-to-sql capabilities of large language models",
        "On the potential of lexico-logical alignments for semantic parsing to sql queries",
        "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
        "TUTA: Treebased transformers for generally structured table pre-training",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}