literature_titles = {
    "Reasoning with Language Model is Planning with World Model": [
        "Working memory",
        "Mental imagery and the varieties of amodal perception",
        "Language models are few-shot learners",
        "Sparks of artificial general intelligence: Early experiments with gpt-4",
        "The computational complexity of propositional strips planning",
        "Model predictive control",
        "Palm: Scaling language modeling with pathways",
        "Training verifiers to solve math word problems",
        "Efficient selectivity and backup operators in monte-carlo tree search",
        "Task and motion planning with large language models for object rearrangement",
        "Designology: Studies on Planning for Action",
        "Mental models",
        "Recurrent world models facilitate policy evolution",
        "World models",
        "Dream to control: Learning behaviors by latent imagination",
        "Mastering atari with discrete world models",
        "Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings",
        "Bertnet: Harvesting knowledge graphs with arbitrary relations from pretrained language models",
        "Control of mental representations in human planning",
        "Towards reasoning in large language models: A survey",
        "Inner monologue: Embodied reasoning through planning with language models",
        "Bonsai trees in your head: how the pavlovian system sculpts goal-directed choices by pruning decision trees",
        "Task planning in robotics: an empirical comparison of pddl-and asp-based systems",
        "Mental models and human reasoning",
        "Mental models: Towards a cognitive science of language, inference, and consciousness",
        "Gpt is becoming a turing machine: Here are some ways to program it",
        "Bandit based monte-carlo planning",
        "Large language models are zero-shot reasoners",
        "A path towards autonomous machine intelligence",
        "Language modeling with latent situations",
        "Llm+ p: Empowering large language models with optimal planning proficiency",
        "Faithful chain-of-thought reasoning",
        "Deep learning, reinforcement learning, and world models",
        "Situations, actions, and causal laws",
        "Augmented language models: a survey",
        "Gpt-4 technical report",
        "Refiner: Reasoning feedback on intermediate representations",
        "Virtualhome: Simulating household activities via programs",
        "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
        "Toolformer: Language models can teach themselves to use tools",
        "Mastering atari, go, chess and shogi by planning with a learned model",
        "Action, perception and the brain: Adaptation and cephalic expression",
        "Planning to explore via self-supervised world models",
        "Reflexion: an autonomous agent with dynamic memory and self-reflection",
        "Mastering chess and shogi by self-play with a general reinforcement learning algorithm",
        "Progprompt: Generating situated robot task plans using large language models",
        "Cognitive maps in rats and men",
        "Llama: Open and efficient foundation language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
        "On the planning abilities of large language models (a critical investigation with a proposed benchmark)",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Generating sequences by learning to self-correct",
        "Daydreamer: World models for physical robot learning",
        "Language models meet world models: Embodied experiences enhance language models",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "Least-to-most prompting enables complex reasoning in large language models",
        "Adaptive information seeking for open-domain question answering"
    ]
}