literature_titles = {
    "SATLM: Satisfiability-Aided Language Models Using Declarative Prompting": [
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Language models are few-shot learners",
        "Evaluating large language models trained on code",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Teaching large language models to self-debug",
        "PaLM: Scaling Language Modeling with Pathways",
        "Training verifiers to solve math word problems",
        "Selection-inference: Exploiting large language models for interpretable logical reasoning",
        "A computing procedure for quantification theory",
        "Z3: An Efficient SMT Solver",
        "Just add functions: A neural-symbolic language model",
        "On the foundations of noise-free selective classification",
        "Complexity-based prompting for multi-step reasoning",
        "Pal: Program-aided language models",
        "Neurosymbolic AI: The 3rd wave",
        "The GEM benchmark: Natural language generation, its evaluation and metrics",
        "Demystifying prompts in language models via perplexity estimation",
        "Solving math word problems by combining language models with symbolic solvers",
        "Jigsaw: Large language models meet program synthesis",
        "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "Large language models are zero-shot reasoners",
        "Explanations from large language models make small reasoners better",
        "On the advance of making language models better reasoners",
        "Holistic evaluation of language models",
        "LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Faithful chain-of-thought reasoning",
        "Self-refine: Iterative refinement with self-feedback",
        "The next decade in AI: four steps towards robust artificial intelligence",
        "LEVER: Learning to Verify Language-to-Code Generation with Execution",
        "Show your work: Scratchpads for intermediate computation with language models",
        "Training language models to follow instructions with human feedback",
        "Refiner: Reasoning feedback on intermediate representations",
        "Synchromesh: Reliable code generation from pre-trained language models",
        "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
        "Multi-modal program inference: A marriage of pre-trained language models and component-based synthesis",
        "A recipe for arbitrary text style transfer with large language models",
        "STREET: A multi-task structured reasoning and explanation benchmark",
        "Multitask prompted training enables zero-shot task generalization",
        "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
        "Toolformer: Language models can teach themselves to use tools",
        "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
        "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
        "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)",
        "Pinto: Faithful language reasoning using prompt-generated rationales",
        "Self-consistency improves chain of thought reasoning in language models",
        "Finetuned language models are zero-shot learners",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "Benchmarking multimodal regex synthesis with complex structures",
        "Optimal neural program synthesis from multimodal specifications",
        "Explanation selection using unlabeled data for chain-of-thought prompting",
        "Generate rather than retrieve: Large language models are strong context generators",
        "Improved logical reasoning of language models via differentiable symbolic programming",
        "OPT: Open Pre-trained Transformer Language Models",
        "Analytical reasoning of text",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}
