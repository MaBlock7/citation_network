literature_titles = {
    "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference": [
        "Synthetic QA corpora generation with roundtrip consistency",
        "Logicguided data augmentation and regularization for consistent question answering",
        "Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms",
        "Can NLI models verify QA systems' predictions?",
        "Transforming question answering datasets into natural language inference datasets",
        "Evaluating coherence in dialogue systems using entailment",
        "Measuring and improving consistency in pretrained language models",
        "TRUE: Re-evaluating factual consistency evaluation",
        "Rc2: an efficient maxsat solver",
        "Maieutic prompting: Logically consistent reasoning with recursive explanations",
        "BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief",
        "ViLT: Vision-and-language transformer without convolution or region supervision",
        "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
        "Natural questions: A benchmark for question answering research",
        "SummaC: Re-visiting NLI-based models for inconsistency detection in summarization",
        "ALBERT: A lite BERT for selfsupervised learning of language representations",
        "A logic-driven framework for consistency of neural models",
        "Roberta: A robustly optimized BERT pretraining approach",
        "An introduction to factor graphs",
        "Modeling semantic containment and exclusion in natural language inference",
        "Fast model editing at scale",
        "Improving factual completeness and consistency of image-to-text radiology report generation",
        "Adversarial NLI: A new benchmark for natural language understanding",
        "Differentiation of blackbox combinatorial solvers",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Know what you don't know: Unanswerable questions for SQuAD",
        "SQuAD: 100,000+ questions for machine comprehension of text",
        "Sunny and dark outside?! improving answer consistency in VQA through entailed question generation",
        "Unsupervised commonsense question answering with self-talk",
        "Editable neural networks",
        "Generating persona consistent dialogues by exploiting natural language inference",
        "Conceptnet 5.5: An open multilingual graph of general knowledge",
        "Generalpurpose question-answering with macaw",
        "LXMERT: Learning cross-modality encoder representations from transformers",
        "Diverse beam search for improved description of complex scenes",
        "Dialogue natural language inference",
        "A broad-coverage challenge corpus for sentence understanding through inference"
    ]
}