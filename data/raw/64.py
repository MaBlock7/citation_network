literature_titles = {
    "PROMPTING GPT-3 TO BE RELIABLE": [
        "Types of out-of-distribution texts and how to detect them",
        "On the dangers of stochastic parrots: Can language models be too big?",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Unobserved local structures make compositional generalization hard",
        "Evaluating the susceptibility of pretrained language models via handcrafted adversarial examples",
        "Verification of forecasts expressed in terms of probability",
        "Language models are few-shot learners",
        "Editing factual knowledge in language models",
        "On the intrinsic and extrinsic fairness evaluation metrics for contextualized language representations",
        "Quantifying memorization across neural language models",
        "Evaluating large language models trained on code",
        "A dataset for answering time-sensitive questions",
        "Palm: Scaling language modeling with pathways",
        "Calibration of pre-trained transformers",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
        "Single-dataset experts for multi-dataset question answering",
        "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
        "Attributed text generation via post-hoc research and revision",
        "Evaluating models' local decision boundaries via contrast sets",
        "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
        "On calibration of modern neural networks",
        "Annotation artifacts in natural language inference data",
        "REALM: Retrieval-augmented language model pre-training",
        "Exploring the role of grammar and word choice in bias toward african american english (aae) in hate speech classification",
        "Pretrained transformers improve out-of-distribution robustness",
        "Unsupervised dense information retrieval with contrastive learning",
        "Few-shot learning with retrieval augmented language models",
        "Adversarial examples for evaluating reading comprehension systems",
        "How can we know when language models know?",
        "Is bert really robust? A strong baseline for natural language attack on text classification and entailment",
        "Language models (mostly) know what they know",
        "Scaling laws for neural language models",
        "Dense passage retrieval for open-domain question answering",
        "RealTime QA: What's the answer right now?",
        "Measuring compositional generalization: A comprehensive method on realistic data",
        "COGS: A compositional generalization challenge based on semantic interpretation",
        "WILDS: A benchmark of in-the-wild distribution shifts",
        "Large language models are zero-shot reasoners",
        "End-to-end neural coreference resolution",
        "The power of scale for parameter-efficient prompt tuning",
        "Zero-shot relation extraction via reading comprehension",
        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
        "D-net: A pretraining and fine-tuning framework for improving the generalization of machine reading comprehension",
        "Bert-attack: Adversarial attack against bert using bert",
        "Teaching models to express their uncertainty in words",
        "Truthfulqa: Measuring how models mimic human falsehoods",
        "Roberta: A robustly optimized bert pretraining approach",
        "An exploration of data augmentation and sampling techniques for domain-agnostic question answering",
        "Entity-based knowledge conflicts in question answering",
        "Gender and representation bias in gpt-3 generated stories",
        "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
        "Reducing conversational agents' overconfidence through linguistic calibration",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Fast model editing at scale",
        "Memory-based model editing at scale",
        "Stereoset: Measuring stereotypical bias in pretrained language models",
        "Obtaining well calibrated probabilities using bayesian binning",
        "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
        "BBQ: A hand-built bias benchmark for question answering",
        "Red teaming language models with language models",
        "Language models as knowledge bases?",
        "Kilt: A benchmark for knowledge intensive language tasks",
        "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods",
        "Hypothesis only baselines in natural language inference",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "Semantically equivalent adversarial rules for debugging nlp models",
        "How much knowledge can you pack into the parameters of a language model?",
        "Gender bias in coreference resolution",
        "What does bert learn from multiple-choice reading comprehension datasets?",
        "Benchmarking robustness of machine reading comprehension models",
        "Better robustness by more coverage: Adversarial training with mixup augmentation for robust fine-tuning",
        "Revisiting calibration for question answering",
        "Process for adapting language models to society (palms) with values-targeted datasets",
        "MultiQA: An empirical investigation of generalization and transfer in reading comprehension",
        "It's morphin' time! combating linguistic discrimination with inflectional perturbations",
        "Reliability testing for natural language processing systems",
        "The risks of machine learning systems",
        "Do multi-hop question answering systems know how to answer the single-hop sub-questions?",
        "FEVER: A large-scale dataset for fact extraction and verification",
        "Glue: A multi-task benchmark and analysis platform for natural language understanding",
        "Adversarial GLUE: A multi-task benchmark for robustness evaluation of language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models",
        "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
        "Can explanations be useful for calibrating black box models?",
        "OPT: Open pre-trained transformer language models",
        "PAWS: Paraphrase adversaries from word scrambling",
        "Gender bias in coreference resolution: Evaluation and debiasing methods",
        "Ethical-advice taker: Do language models understand natural language interventions?",
        "Calibrate before use: Improving few-shot performance of language models",
        "Value: Understanding dialect disparity in nlu"
    ]
}