literature_titles = {
    "Synthetic prompting: generating chain-of-thought demonstrations for large language models": [
        "Language models are few-shot learners",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Palm: Scaling language modeling with pathways",
        "Scaling instruction-finetuned language models",
        "Training verifiers to solve math word problems",
        "Compositional semantic parsing with large language models",
        "Complexity-based prompting for multi-step reasoning",
        "Rarr: Researching and revising what language models say, using language models",
        "PAL: program-aided language models",
        "The curious case of neural text degeneration",
        "Large language models can self-improve",
        "Decomposed prompting: A modular approach for solving complex tasks",
        "Large language models are zero-shot reasoners",
        "MAWPS: A math word problem repository",
        "On the advance of making language models better reasoners",
        "WANLI: worker and AI collaboration for natural language inference dataset creation",
        "What makes good in-context examples for gpt-3?",
        "Z-ICL: zero-shot in-context learning with pseudodemonstrations",
        "A diverse corpus for evaluating and developing english math word problem solvers",
        "Training language models to follow instructions with human feedback",
        "Are NLP models really able to solve simple math word problems?",
        "Reasoning like program executors",
        "Measuring and narrowing the compositionality gap in language models",
        "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
        "Multitask prompted training enables zero-shot task generalization",
        "On the effect of pretraining corpora on in-context learning by a large-scale language model",
        "Challenging big-bench tasks and whether chain-of-thought can solve them",
        "Lamda: Language models for dialog applications",
        "Self-consistency improves chain of thought reasoning in language models",
        "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
        "Finetuned language models are zero-shot learners",
        "Chain of thought prompting elicits reasoning in large language models",
        "Symbolic knowledge distillation: from general language models to commonsense models",
        "DOC: improving long story coherence with detailed outline control",
        "Zerogen: Efficient zero-shot learning via dataset generation",
        "Complementary explanations for effective in-context learning",
        "OPT: open pre-trained transformer language models",
        "Automatic chain of thought prompting in large language models",
        "Least-to-most prompting enables complex reasoning in large language models"
    ]
}