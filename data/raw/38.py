literature_titles = {
    "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery": [
        "Language Models are Few-Shot Learners",
        "Reproducible scaling laws for contrastive language-image learning",
        "Scaling instruction-finetuned language models",
        "Binaryconnect: Training deep neural networks with binary weights during propagations",
        "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1",
        "Discovering the hidden vocabulary of dalle-2",
        "RLPrompt: Optimizing discrete text prompts with reinforcement learning",
        "OpenPrompt: An open-source framework for prompt-learning",
        "HotFlip: White-box adversarial examples for text classification",
        "An image is worth one word: Personalizing text-to-image generation using textual inversion",
        "Gradient-based adversarial attacks against text transformers",
        "Scaling up vision-language pre-training for image captioning",
        "Prompt waywardness: The curious case of discretized interpretation of continuous prompts",
        "Gradient-based constrained sampling from language models",
        "The power of scale for parameter-efficient prompt tuning",
        "Training quantized nets: A deeper understanding",
        "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "Microsoft coco: Common objects in context",
        "Deep learning face attributes in the wild",
        "Decoupled weight decay regularization",
        "Hidden factors and hidden topics: Understanding rating dimensions with review text",
        "Grips: Gradient-free, edit-based instruction search for prompting large language models",
        "Multitask prompted training enables zero-shot task generalization",
        "Laion-5b: An open large-scale dataset for training next generation image-text models",
        "Adafactor: Adaptive learning rates with sublinear memory cost",
        "Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?",
        "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Vinvl: Revisiting visual representations in vision-language models",
        "Opt: Open pre-trained transformer language models",
        "Character-level convolutional networks for text classification"
    ]
}